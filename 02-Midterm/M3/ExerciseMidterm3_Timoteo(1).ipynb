{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YBGo0PWtrGhs"
      },
      "outputs": [],
      "source": [
        "!pip install transformers -q\n",
        "!pip install torch -q"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This cell imports the specific modules and classes we'll need for our script.\n",
        "\n",
        "from transformers import pipeline, BertForQuestionAnswering, BertTokenizer:\n",
        "\n",
        "pipeline: A high-level, easy-to-use API from the Hugging Face library that simplifies the process of using models for specific tasks (like question-answering).\n",
        "\n",
        "BertForQuestionAnswering: The specific BERT model architecture designed for question-answering tasks.\n",
        "\n",
        "BertTokenizer: The tokenizer that corresponds to the BERT model, responsible for converting text into a format the model can understand (tokens).\n",
        "\n",
        "import textwrap: A standard Python library used here to format our long context paragraph for cleaner printing.\n",
        "\n",
        "import time: A standard Python library used to measure the inference time (how long it takes for the model to generate an answer).\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "**Reflections and Observations**\n",
        "\n",
        "By importing these specific components, we are setting up the building blocks for our Q&A system. While the pipeline function is powerful enough to handle model and tokenizer loading on its own, explicitly importing BertForQuestionAnswering and BertTokenizer helps in understanding the underlying components that make the pipeline work. Measuring performance is crucial, so importing the time library from the start was a key step in planning our model evaluation."
      ],
      "metadata": {
        "id": "eBPYukECU0Wa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "from transformers import pipeline, BertForQuestionAnswering, BertTokenizer\n",
        "import textwrap\n",
        "import time"
      ],
      "metadata": {
        "id": "IqJ7aiIksKPX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Code Explanation\n",
        "\n",
        "This cell downloads and initializes our first question-answering model.\n",
        "\n",
        "We define model_name as 'deepset/bert-base-cased-squad2'. This is an identifier for a specific model hosted on the Hugging Face Hub. This particular model is a BERT base model that has been fine-tuned on the SQuAD 2.0 dataset, which is a benchmark for question-answering.\n",
        "\n",
        "BertForQuestionAnswering.from_pretrained(model_name) and BertTokenizer.from_pretrained(model_name) download the necessary model weights and tokenizer files.\n",
        "\n",
        "pipeline('question-answering', ...) creates a convenient object (qna_pipeline) that takes care of all the preprocessing, model inference, and post-processing steps.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "Output Explanation\n",
        "\n",
        "The output shows the progress bars for downloading the model's configuration (config.json), the model weights (model.safetensors), and the tokenizer files (tokenizer_config.json, vocab.txt, etc.). The final line confirms that the model was loaded successfully. The warning about \"Some weights of the model checkpoint... were not used\" is expected and simply means that parts of the pre-trained model not needed for question-answering were discarded.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "**Reflections and Observations**\n",
        "\n",
        "This model serves as our baseline. It's a standard and widely-used model, making it a great starting point for comparison. The \"deepset\" prefix indicates it's a version provided by the company Deepset, known for their work in NLP. The download size is substantial (~433MB), which is typical for BERT-base models. The real magic is in the pipeline function, which abstracts away a lot of complex code and lets us focus on the task itself.\n"
      ],
      "metadata": {
        "id": "xJxSNOQ9VOAi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the model name from the Hugging Face Hub\n",
        "model_name = 'deepset/bert-base-cased-squad2'\n",
        "\n",
        "# Load the pre-trained model and its tokenizer\n",
        "# The from_pretrained() method downloads and caches the model files\n",
        "model = BertForQuestionAnswering.from_pretrained(model_name)\n",
        "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# Create the question-answering pipeline\n",
        "# This pipeline bundles the model and tokenizer for easy use\n",
        "qna_pipeline = pipeline('question-answering', model=model, tokenizer=tokenizer)\n",
        "\n",
        "print(f\"Model '{model_name}' loaded successfully.\")"
      ],
      "metadata": {
        "id": "QOH7_qDYsSbZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Code Explanation\n",
        "\n",
        "In this cell, we define the \"context\" that the model will use to find answers.\n",
        "\n",
        "A multi-line string is used to store a news article about the repatriation of Filipino workers from Lebanon. This text will be the sole source of information for the model.\n",
        "\n",
        "textwrap.dedent(context).strip() is used to clean up the string by removing any common leading whitespace from each line.\n",
        "\n",
        "textwrap.fill(...) formats the cleaned text to have a maximum line width of 100 characters, making it easy to read in the output.\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "**Reflections and Observations**\n",
        "\n",
        "I think the quality and content of the context are critical. The model's performance is entirely dependent on this text. it cannot use any external knowledge. This specific article was chosen by Sir Raga, probably because it contains a good mix of names, numbers, locations, and reasons, which allows for a diverse set of test questions."
      ],
      "metadata": {
        "id": "c4T5NhBGVogJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# The full text from the article will serve as our context\n",
        "# Triple quotes (\"\"\") are used for multi-line strings in Python\n",
        "context = \"\"\"\n",
        "MANILA – The government is arranging chartered flights for the repatriation of more than 200 overseas Filipino workers in Beirut, Lebanon, the Department of Migrant Workers (DMW) said Wednesday. “We are trying to provide for chartered flights. We're talking to airline companies so that the chartered flights would be able to accommodate for example, no less than 300 overseas Filipino workers from Beirut,” DMW Undersecretary Bernard Olalia said in a Palace press briefing. This was after the scheduled flights of around 15 OFWs on Sept. 25 were cancelled because of the recent bombings in Beirut. Olalia said around 111 OFWs are staying in four temporary shelters in Beirut and waiting for their repatriation. An additional 110 OFWs are applying for exit permits from the Lebanese government, Olalia said. “Apart from the documented OFWs, we have undocumented OFWs who need to secure travel documents and once they're given travel documents, we will help them in securing also exit visas or exit permits from the Immigration of the Lebanese government,” he said. Olalia, however, said the Philippine government is facing several challenges, including securing landing rights for chartered flights. He said land and sea routes are being considered, in case the situation escalates and makes it “impossible” to take the air route. “The DMW is also studying the possibility of other routes. Apart from air route, we will be assessing the sea and the land route, should the case or the situation there worsen,” Olalia said. He said the DMW, the Overseas Workers Welfare Administration (OWWA), and other concerned agencies will adopt a “whole-of-government assistance\" upon the directive of President Ferdinand R. Marcos Jr. He said each repatriated OFW will get PHP150,000 in financial assistance from the DMW and OWWA, as well as psychosocial services. Israel has intensified its airstrikes across the northern border into Lebanon, targeting the Iran-backed militant group Hezbollah. Iran fired ballistic missiles in Israel on Tuesday night, following the deadly attacks on Gaza and Lebanon and the recent killings of Hamas, Hezbollah, and Islamic Revolutionary Guard Corps leaders. Olalia said no Filipinos were hurt since the attacks were launched. “We have men on the ground. They work around the clock. At 'yung mga staff po natin, dinagdagan na po natin (And we augmented our staff) both in Lebanon at (and) nearby posts to be able to provide safest route, to evacuate and ultimately to facilitate the repatriation of our OFWs both either in Lebanon or in Israel,” he said. (PNA)\n",
        "\"\"\"\n",
        "\n",
        "# Format the context for clean printing\n",
        "# .strip() removes any leading/trailing whitespace\n",
        "dedented_text = textwrap.dedent(context).strip()\n",
        "\n",
        "# Print the context to verify it's loaded correctly\n",
        "print(\"Context Article:\\n\")\n",
        "print(textwrap.fill(dedented_text, width=100)) # 'width' adjusts the line length"
      ],
      "metadata": {
        "id": "WHOHQWM7seBS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Code Explanation\n",
        "\n",
        "This cell creates an interactive loop to test the model.\n",
        "\n",
        "A while True loop runs continuously.\n",
        "\n",
        "input() prompts the user to enter a question.\n",
        "\n",
        "If the user enters *, the break statement exits the loop.\n",
        "\n",
        "time.time() records the start time right before the model processes the input.\n",
        "\n",
        "qna_pipeline(...) takes the user's question and the context as input and returns a dictionary containing the predicted answer.\n",
        "\n",
        "The end time is recorded, and the difference is calculated to get the inference_time.\n",
        "\n",
        "The answer, its confidence score, and the inference time are printed.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "Output Explanation\n",
        "\n",
        "The output displays a series of dialogues where a question is asked and the model provides an answer, a confidence score, and the time it took. For example:\n",
        "\n",
        "Question: Who is the DMW Undersecretary mentioned in the press briefing?\n",
        "\n",
        "Answer: Bernard Olalia\n",
        "\n",
        "Score: 0.9981 (Very high confidence)\n",
        "\n",
        "Inference Time: 3.7034 seconds\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "**Reflections and Observations**\n",
        "\n",
        "This interactive session is where we see the model's capabilities firsthand.\n",
        "\n",
        "Accuracy: The model performed quite well, correctly identifying names (Bernard Olalia), numbers (111), locations (Beirut), and reasons (recent bombings). However, it made a notable mistake on the last question, identifying \"Bernard Olalia\" as the source of the directive instead of \"President Ferdinand R. Marcos Jr.\" This highlights that even powerful models can make errors.\n",
        "\n",
        "Confidence Score: The scores generally correlate with the answer's quality. The 0.9981 score for \"Bernard Olalia\" was very high and correct. The score of 0.1991 for the reason for flight cancellations was low, but the answer was still correct, indicating the model was less certain.\n",
        "\n",
        "Inference Speed: The inference time for this baseline BERT model is around 3.7 to 4.6 seconds per question. This is a crucial metric we will use to compare against other models."
      ],
      "metadata": {
        "id": "mLfVWLQ2V-j0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This loop will continue asking for questions until you enter '*'\n",
        "while True:\n",
        "    inquiry = input(\"\\nType your question (or enter '*' to stop): \")\n",
        "\n",
        "    if inquiry == '*':\n",
        "        break\n",
        "\n",
        "    # --- Measure Inference Time ---\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Feed the question and context to the pipeline\n",
        "    answer = qna_pipeline({'question': inquiry, 'context': context})\n",
        "\n",
        "    end_time = time.time()\n",
        "    inference_time = end_time - start_time\n",
        "    # -----------------------------\n",
        "\n",
        "    # The pipeline returns a dictionary. Let's print the details.\n",
        "    print(f\"\\nAnswer: {answer['answer']}\")\n",
        "    print(f\"Score (Confidence): {answer['score']:.4f}\")\n",
        "    print(f\"Inference Time: {inference_time:.4f} seconds\")"
      ],
      "metadata": {
        "id": "r1cBPadbsouL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Code Explanation\n",
        "\n",
        "This cell loads our second model. The process is identical to the first, but we are now using a different model name: 'distilbert-base-cased-distilled-squad'. This time, we let the pipeline function handle the loading of both the model and the tokenizer by passing the model name string directly.\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "**Reflections and Observations**\n",
        "\n",
        "The key difference here is the model itself. DistilBERT is a \"distilled\" version of BERT. This means it's a smaller, faster, and lighter model that was trained to mimic the behavior of the larger BERT model. My hypothesis is that this model will have a significantly faster inference time, but we might see a slight drop in accuracy or confidence as a trade-off. Its download size (~261MB) is much smaller than the first model's, reinforcing its \"lightweight\" nature."
      ],
      "metadata": {
        "id": "f6k-d4T0WnST"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- MODEL 2: distilbert-base-cased-distilled-squad ---\n",
        "\n",
        "# Define the model name\n",
        "model_name_2 = 'distilbert-base-cased-distilled-squad'\n",
        "\n",
        "# Create the pipeline for this model\n",
        "qna_pipeline_2 = pipeline('question-answering', model=model_name_2, tokenizer=model_name_2)\n",
        "print(f\"Model '{model_name_2}' loaded successfully.\")"
      ],
      "metadata": {
        "id": "4i-t3LMCwRFb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Code Explanation\n",
        "\n",
        "This cell uses the same interactive loop as before but now calls qna_pipeline_2 to get answers from the DistilBERT model. This allows for a direct, one-to-one comparison with Model 1 using the same set of questions.\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "Output Explanation\n",
        "\n",
        "The output shows the answers from DistilBERT for the same 10 questions.\n",
        "\n",
        "Question: Whose directive led to the \"whole-of-government assistance\"?\n",
        "\n",
        "Answer: President Ferdinand R. Marcos Jr\n",
        "\n",
        "Score: 0.5624\n",
        "\n",
        "Inference Time: 1.1969 seconds\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "**Reflections and Observations**\n",
        "\n",
        "This is where our comparison gets interesting.\n",
        "\n",
        "Accuracy: DistilBERT correctly answered all the questions, including the one that the baseline BERT model got wrong! It correctly identified \"President Ferdinand R. Marcos Jr.\" as the source of the directive. This is an impressive result for a smaller model.\n",
        "\n",
        "Confidence Score: The scores were generally high and reliable. It's interesting to note that some scores are displayed as greater than 1.0; this is an artifact of the model's raw output (logits) and how the pipeline sometimes scales them. The relative values are still meaningful.\n",
        "\n",
        "Inference Speed: As hypothesized, DistilBERT is significantly faster. The inference times are mostly in the 1.2 to 2.6 second range, which is roughly 2-3 times faster than the first model. This confirms its efficiency.\n",
        "\n",
        "For applications where speed is critical without a major sacrifice in accuracy, DistilBERT appears to be an excellent choice."
      ],
      "metadata": {
        "id": "IdbtFMtcW4Bp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Testing Loop for Model 2 ---\n",
        "print(f\"--- Now testing model: {model_name_2} ---\")\n",
        "\n",
        "while True:\n",
        "    inquiry = input(\"\\nType your question (or enter '*' to stop): \")\n",
        "    if inquiry == '*':\n",
        "        break\n",
        "\n",
        "    start_time = time.time()\n",
        "    # IMPORTANT: Use the correct pipeline variable for this model\n",
        "    answer = qna_pipeline_2({'question': inquiry, 'context': context})\n",
        "    end_time = time.time()\n",
        "\n",
        "    inference_time = end_time - start_time\n",
        "\n",
        "    print(f\"\\nAnswer: {answer['answer']}\")\n",
        "    print(f\"Score (Confidence): {answer['score']:.4f}\")\n",
        "    print(f\"Inference Time: {inference_time:.4f} seconds\")"
      ],
      "metadata": {
        "id": "YHiUZzxswccF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Code Explanation\n",
        "\n",
        "These cells load and test our third model, RoBERTa (deepset/roberta-base-squad2). RoBERTa (A Robustly Optimized BERT Pretraining Approach) is a variation of BERT that was trained with an improved methodology, including training on a much larger dataset for a longer time.\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "Output Explanation\n",
        "\n",
        "The output shows the answers from RoBERTa for our standard set of questions.\n",
        "\n",
        "Question: According to Undersecretary Olalia, were any Filipinos hurt in the attacks?\n",
        "\n",
        "Answer: no\n",
        "\n",
        "Score: 0.2952\n",
        "\n",
        "Inference Time: 2.2756 seconds\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "**Reflections and Observations**\n",
        "\n",
        "Accuracy: RoBERTa also performed perfectly, correctly answering all questions. Its answer to the question about Filipino casualties was just \"no,\" which is more concise and natural than the first model's \"no Filipinos were hurt.\"\n",
        "\n",
        "Confidence Score: The confidence scores were solid. The score for \"no\" was low (0.2952), but the answer was correct, again showing that low scores don't always mean wrong answers.\n",
        "\n",
        "Inference Speed: The inference times were mostly in the 2.2 to 3.3 second range. This makes RoBERTa faster than our baseline BERT model but slightly slower than the highly optimized DistilBERT.\n",
        "\n",
        "RoBERTa seems to offer a great balance of high accuracy and reasonable speed, making it a very strong contender."
      ],
      "metadata": {
        "id": "_oC_qohdXEHx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- MODEL 3: deepset/roberta-base-squad2 (Corrected Name) ---\n",
        "\n",
        "# Define the correct model name\n",
        "model_name_3 = 'deepset/roberta-base-squad2'\n",
        "\n",
        "# Create the pipeline for this model\n",
        "qna_pipeline_3 = pipeline('question-answering', model=model_name_3, tokenizer=model_name_3)\n",
        "\n",
        "print(f\"Model '{model_name_3}' loaded successfully.\")"
      ],
      "metadata": {
        "id": "Uw4cvbO3yRHi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Testing Loop for Model 3 ---\n",
        "print(f\"--- Now testing model: {model_name_3} ---\")\n",
        "\n",
        "# Run the interactive loop to ask your 10 questions\n",
        "while True:\n",
        "    inquiry = input(\"\\nType your question (or enter '*' to stop): \")\n",
        "    if inquiry == '*':\n",
        "        break\n",
        "\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Use the pipeline variable specific to this model: qna_pipeline_3\n",
        "    answer = qna_pipeline_3({'question': inquiry, 'context': context})\n",
        "\n",
        "    end_time = time.time()\n",
        "    inference_time = end_time - start_time\n",
        "\n",
        "    # Print the results for recording\n",
        "    print(f\"\\nAnswer: {answer['answer']}\")\n",
        "    print(f\"Score (Confidence): {answer['score']:.4f}\")\n",
        "    print(f\"Inference Time: {inference_time:.4f} seconds\")"
      ],
      "metadata": {
        "id": "3CgTzSa5yTPL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Testing Loop for Model 3 ---\n",
        "print(f\"--- Now testing model: {model_name_3} ---\")\n",
        "\n",
        "# Run the interactive loop to ask your 10 questions\n",
        "while True:\n",
        "    inquiry = input(\"\\nType your question (or enter '*' to stop): \")\n",
        "    if inquiry == '*':\n",
        "        break\n",
        "\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Use the pipeline variable specific to this model: qna_pipeline_3\n",
        "    answer = qna_pipeline_3({'question': inquiry, 'context': context})\n",
        "\n",
        "    end_time = time.time()\n",
        "    inference_time = end_time - start_time\n",
        "\n",
        "    # Print the results for recording\n",
        "    print(f\"\\nAnswer: {answer['answer']}\")\n",
        "    print(f\"Score (Confidence): {answer['score']:.4f}\")\n",
        "    print(f\"Inference Time: {inference_time:.4f} seconds\")"
      ],
      "metadata": {
        "id": "wcJCar7IytDG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Code Explanation\n",
        "\n",
        "These final cells load and test our largest model, 'deepset/bert-large-uncased-whole-word-masking-squad2'.\n",
        "\n",
        "bert-large: This model has more layers and parameters than the bert-base models, making it more powerful but also more computationally expensive.\n",
        "\n",
        "whole-word-masking: This refers to a specific pre-training technique that can improve a model's understanding of language.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "Output Explanation\n",
        "\n",
        "The output shows the answers from the BERT-large model.\n",
        "\n",
        "Question: How much financial assistance will each repatriated OFW receive?\n",
        "\n",
        "Answer: PHP15\n",
        "\n",
        "Score: 0.9909\n",
        "\n",
        "Inference Time: 8.3573 seconds\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "Reflections and Observations\n",
        "\n",
        "Accuracy: Surprisingly, this larger model made an error! For the financial assistance question, it answered PHP15 instead of PHP150,000. This is a critical failure and a great lesson: bigger is not always better. The way the tokenizer splits \"PHP150,000\" might have confused this specific model. Otherwise, its other answers were correct.\n",
        "\n",
        "Confidence Score: The scores were generally very high, but this can be misleading, as shown by the incorrect answer with a score of 0.9909. This highlights the importance of not relying on confidence scores alone.\n",
        "\n",
        "Inference Speed: As expected, this model was by far the slowest. Inference times were consistently in the 8 to 13 second range, making it more than twice as slow as the baseline BERT model and nearly 8 times slower than DistilBERT.\n",
        "\n"
      ],
      "metadata": {
        "id": "ZLpO7zbFXcy2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- MODEL 4: deepset/bert-large-uncased-whole-word-masking-squad2 ---\n",
        "\n",
        "# Define the correct model name, including the 'deepset/' prefix\n",
        "model_name_4 = 'deepset/bert-large-uncased-whole-word-masking-squad2'\n",
        "\n",
        "# Create the pipeline for this final model.\n",
        "# This model is significantly larger than the others, so the download may be over 1GB.\n",
        "qna_pipeline_4 = pipeline('question-answering', model=model_name_4, tokenizer=model_name_4)\n",
        "\n",
        "print(f\"Model '{model_name_4}' loaded successfully.\")"
      ],
      "metadata": {
        "id": "dH-boWxo0uQ8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Testing Loop for Model 4 ---\n",
        "print(f\"--- Now testing model: {model_name_4} ---\")\n",
        "\n",
        "# Run the interactive loop to ask your 10 questions\n",
        "while True:\n",
        "    inquiry = input(\"\\nType your question (or enter '*' to stop): \")\n",
        "    if inquiry == '*':\n",
        "        break\n",
        "\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Use the pipeline variable for this model: qna_pipeline_4\n",
        "    answer = qna_pipeline_4({'question': inquiry, 'context': context})\n",
        "\n",
        "    end_time = time.time()\n",
        "    inference_time = end_time - start_time\n",
        "\n",
        "    # Print the results for recording\n",
        "    print(f\"\\nAnswer: {answer['answer']}\")\n",
        "    print(f\"Score (Confidence): {answer['score']:.4f}\")\n",
        "    print(f\"Inference Time: {inference_time:.4f} seconds\")"
      ],
      "metadata": {
        "id": "enmZW_cN0wj4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "My Final Conclusion\n",
        "\n",
        "This activity was a fascinating comparative study of different transformer architectures for question-answering.\n",
        "\n",
        "Baseline (BERT-base): Performed well but was the second slowest and made one clear error.\n",
        "\n",
        "Efficient (DistilBERT): The standout performer. It was the fastest by a large margin and was 100% accurate on the test questions. This makes it an ideal choice for production environments where speed is important.\n",
        "\n",
        "Robust (RoBERTa): Also 100% accurate and faster than the baseline BERT. It provides a fantastic balance of speed and reliability.\n",
        "\n",
        "Large (BERT-large): The slowest and, surprisingly, not the most accurate in this test. Its failure on a simple number-based question shows that even the most complex models have weaknesses.\n",
        "\n",
        "Overall, for this specific task, DistilBERT provided the best combination of speed and accuracy, proving that a lighter, well-optimized model can outperform its larger counterparts."
      ],
      "metadata": {
        "id": "b_bOPwAtZCjy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "path = \"/content/ExerciseMidterm3_Timoteo.ipynb\"\n",
        "\n",
        "with open(path, \"r\") as f:\n",
        "    nb = json.load(f)\n",
        "\n",
        "# Remove broken widget metadata\n",
        "if \"widgets\" in nb.get(\"metadata\", {}):\n",
        "    del nb[\"metadata\"][\"widgets\"]\n",
        "\n",
        "with open(path, \"w\") as f:\n",
        "    json.dump(nb, f)\n"
      ],
      "metadata": {
        "id": "p-8jKn0HwGyr"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}