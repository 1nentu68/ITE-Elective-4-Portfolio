{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# PART 1: Text Classification"
      ],
      "metadata": {
        "id": "XstP4pemjAqX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.1: Preparing Both Manual and Sourced Datasets\n",
        "\n",
        "\n",
        "*   **Function Description**: This block serves as the complete data preparation pipeline for our experiment. It prepares two independent sets of data. First, it defines and splits a small, manually-created dataset of 20 samples. Second, it mounts Google Drive, loads the large, externally-sourced `emails.csv` dataset, processes it into the required format, and splits it. The purpose is to create two distinct pairs of training and testing sets (`manual_train_set`/`manual_test_set` and `sourced_train_set`/`sourced_test_set`) that will be used to train and evaluate two separate models in the subsequent blocks.\n",
        "\n",
        "*   **Data Source References**:\n",
        "    1.  **Manually Created Data**: A custom set of 20 email-style messages (10 spam, 10 ham) created to fulfill the manual labeling requirement of the assignment.\n",
        "    2.  **Independently Sourced Data**:\n",
        "        *   **Dataset Name**: `emails.csv`\n",
        "        *   **Source**: User's personal Google Drive.\n",
        "        *   **Location**: `My Drive/ITC508_data/`\n",
        "        *   **Description**: A CSV file containing thousands of emails. The columns `Text` and `Spam` represent the email content and its classification label, respectively.\n",
        "\n",
        "*   **Syntax Explanation**:\n",
        "    *   `manual_data = [...]`: A hard-coded Python list containing 20 tuples, where each tuple holds a text string and its corresponding annotation dictionary.\n",
        "    *   `manual_train_set = manual_data[:16]`: This list slice creates the training set for the manual data, taking the first 16 samples (80% of 20).\n",
        "    *   `drive.mount(...)`: A function from the `google.colab` library that establishes a connection to the user's Google Drive, making its contents accessible.\n",
        "    *   `pd.read_csv(file_path)`: A pandas function that loads the `emails.csv` file from the specified Google Drive path into a DataFrame.\n",
        "    *   `df.iterrows()`: A pandas method to iterate over each row of the DataFrame, allowing access to the `Text` and `Spam` columns for each email.\n",
        "\n",
        "*   **Inputs**:\n",
        "    1.  The hard-coded `manual_data` list.\n",
        "    2.  The `emails.csv` file located in the user's Google Drive at `/ITC508_data/`.\n",
        "    3.  User authorization is required via a pop-up to mount Google Drive.\n",
        "\n",
        "*   **Outputs**:\n",
        "    1.  `manual_train_set`: A list of 16 manually-created samples for training the first model.\n",
        "    2.  `manual_test_set`: A list of 4 manually-created samples for testing the first model.\n",
        "    3.  `sourced_train_set`: A list containing 80% of the samples from `emails.csv` for training the second model.\n",
        "    4.  `sourced_test_set`: A list containing 20% of the samples from `emails.csv` for testing the second model.\n",
        "\n",
        "*   **Code Flow**: The code executes in two distinct parts. First, it defines the `manual_data` list, shuffles it, and splits it into training and testing sets. Second, it connects to Google Drive, loads, processes, shuffles, and splits the `sourced_data` into its own training and testing sets. The block concludes by making all four data variables available for the next steps.\n",
        "\n",
        "*   **Comments and Observations**: In this initial setup, I prepared two distinct datasets for training a spam detector. First, I created a small, manual list of 20 clear-cut spam and ham examples to ensure the model learns from some textbook cases; I then shuffled and split this into a tiny training and testing set. The more significant part involved mounting my Google Drive to access a larger emails.csv file using the pandas library. I looped through this sourced data, converting its binary spam labels (1 or 0) into the same categorical dictionary format as my manual data, which seems necessary for the machine learning model I'll use later. After ensuring all text entries were valid strings, I performed a seeded, reproducible shuffle on this larger dataset and split it into a more robust 80/20 division for training and testing, giving me a substantial amount of data to build a more effective model."
      ],
      "metadata": {
        "id": "fw3IH8zfMa6K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import random\n",
        "import os\n",
        "from google.colab import drive\n",
        "\n",
        "# --- Part 1: Prepare Manually Labeled Dataset ---\n",
        "manual_data = [\n",
        "    # -- SPAM Examples (10) --\n",
        "    (\"Claim your free prize now!\", {\"cats\": {\"SPAM\": 1.0, \"HAM\": 0.0}}),\n",
        "    (\"URGENT: Your account needs immediate attention\", {\"cats\": {\"SPAM\": 1.0, \"HAM\": 0.0}}),\n",
        "    (\"Click here to win a new iPhone\", {\"cats\": {\"SPAM\": 1.0, \"HAM\": 0.0}}),\n",
        "    (\"Exclusive offer just for you, limited time only\", {\"cats\": {\"SPAM\": 1.0, \"HAM\": 0.0}}),\n",
        "    (\"Congratulations! You have been selected for a special reward\", {\"cats\": {\"SPAM\": 1.0, \"HAM\": 0.0}}),\n",
        "    (\"$$$ Make money fast with this simple trick\", {\"cats\": {\"SPAM\": 1.0, \"HAM\": 0.0}}),\n",
        "    (\"Your payment is overdue, please update your details\", {\"cats\": {\"SPAM\": 1.0, \"HAM\": 0.0}}),\n",
        "    (\"Meet local singles tonight\", {\"cats\": {\"SPAM\": 1.0, \"HAM\": 0.0}}),\n",
        "    (\"You've won the lottery, click to claim\", {\"cats\": {\"SPAM\": 1.0, \"HAM\": 0.0}}),\n",
        "    (\"We need to verify your bank account information\", {\"cats\": {\"SPAM\": 1.0, \"HAM\": 0.0}}),\n",
        "\n",
        "    # -- HAM (Not Spam) Examples (10) --\n",
        "    (\"Hello, are we still on for the meeting tomorrow?\", {\"cats\": {\"SPAM\": 0.0, \"HAM\": 1.0}}),\n",
        "    (\"Here is the report you requested\", {\"cats\": {\"SPAM\": 0.0, \"HAM\": 1.0}}),\n",
        "    (\"Can you please review this document?\", {\"cats\": {\"SPAM\": 0.0, \"HAM\": 1.0}}),\n",
        "    (\"Thanks for your email, I will get back to you shortly\", {\"cats\": {\"SPAM\": 0.0, \"HAM\": 1.0}}),\n",
        "    (\"Let's catch up for lunch next week\", {\"cats\": {\"SPAM\": 0.0, \"HAM\": 1.0}}),\n",
        "    (\"Attached is the new project schedule\", {\"cats\": {\"SPAM\": 0.0, \"HAM\": 1.0}}),\n",
        "    (\"What time is the team call today?\", {\"cats\": {\"SPAM\": 0.0, \"HAM\": 1.0}}),\n",
        "    (\"See you at the conference!\", {\"cats\": {\"SPAM\": 0.0, \"HAM\": 1.0}}),\n",
        "    (\"Your Amazon order has shipped\", {\"cats\": {\"SPAM\": 0.0, \"HAM\": 1.0}}),\n",
        "    (\"I'm running about 10 minutes late, apologies\", {\"cats\": {\"SPAM\": 0.0, \"HAM\": 1.0}})\n",
        "]\n",
        "random.shuffle(manual_data)\n",
        "manual_train_set = manual_data[:16] # 80% of 20 is 16\n",
        "manual_test_set = manual_data[16:]  # The remaining 4\n",
        "print(f\"Manual data prepared: {len(manual_train_set)} training samples, {len(manual_test_set)} testing samples.\")\n",
        "print(\"-\" * 30)\n",
        "\n",
        "\n",
        "# --- Part 2: Prepare Sourced Dataset from Google Drive ---\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "file_name = 'emails.csv'\n",
        "file_path = f'/content/drive/My Drive/ITC508_data/{file_name}'\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "sourced_data = []\n",
        "for index, row in df.iterrows():\n",
        "    text = row['Text']\n",
        "    label = row['Spam']\n",
        "    if label == 1:\n",
        "        annotations = {\"cats\": {\"SPAM\": 1.0, \"HAM\": 0.0}}\n",
        "    else:\n",
        "        annotations = {\"cats\": {\"SPAM\": 0.0, \"HAM\": 1.0}}\n",
        "    if isinstance(text, str):\n",
        "        sourced_data.append((text, annotations))\n",
        "\n",
        "random.seed(42)\n",
        "random.shuffle(sourced_data)\n",
        "split_point = int(len(sourced_data) * 0.8)\n",
        "sourced_train_set = sourced_data[:split_point]\n",
        "sourced_test_set = sourced_data[split_point:]\n",
        "print(f\"Sourced data prepared: {len(sourced_train_set)} training samples, {len(sourced_test_set)} testing samples.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4G6nI937MZBn",
        "outputId": "7b367aa9-e522-4fb1-e22d-469a75075fb1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Manual data prepared: 16 training samples, 4 testing samples.\n",
            "------------------------------\n",
            "Mounted at /content/drive\n",
            "Sourced data prepared: 4582 training samples, 1146 testing samples.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.2: Model Trained on Manual Data\n",
        "\n",
        "\n",
        "*   **Function Description**: This block focuses entirely on the manually-created dataset. It initializes a new spaCy model (`nlp_manual`), trains it exclusively on the 16 samples in `manual_train_set`, and then evaluates its performance on the 4 unseen samples in `manual_test_set`. The core training logic is taken directly from the professor's sample code. Finally, it provides an interactive loop for the user to test this specific, minimally-trained model with their own inputs.\n",
        "\n",
        "*   **Syntax Explanation**:\n",
        "    *   **`# --- PROVIDED SAMPLE CODE ...`**: These comments clearly mark the sections of code that directly implement the logic from the professor's provided snippet.\n",
        "    *   `nlp_manual = spacy.blank(\"en\")`: Creates a new, empty English language model.\n",
        "    *   `nlp_manual.add_pipe(\"textcat\")`: Adds the text classification component to the model.\n",
        "    *   `nlp_manual.initialize()`: Prepares the model's weights for training.\n",
        "    *   `nlp_manual.update([example], sgd=optimizer)`: The core training step from the sample code. It shows the model one example and adjusts its weights to reduce error.\n",
        "    *   `for i in range(10):`: This is an added outer loop (an \"epoch\" loop). It runs the training process 10 times over the small dataset to give the model a better chance to learn.\n",
        "    *   `classification_report(...)`: An added function from scikit-learn that calculates detailed performance metrics (precision, recall, F1-score), as required by the assignment.\n",
        "    *   `while True:`: The interactive testing loop from the sample code, allowing for live classification of user input.\n",
        "\n",
        "*   **Inputs**:\n",
        "    1.  `manual_train_set`: The list of 16 training examples from Block 1.\n",
        "    2.  `manual_test_set`: The list of 4 testing examples from Block 1.\n",
        "    3.  `user_input`: Text entered by the user during the interactive testing phase.\n",
        "\n",
        "*   **Outputs**:\n",
        "    1.  A printed log showing the start and end of the training process.\n",
        "    2.  An accuracy score and a detailed Classification Report for the model's performance on the manual test set.\n",
        "    3.  An interactive prompt that takes user input and prints the model's classification (`SPAM` or `HAM`).\n",
        "\n",
        "*   **Code Flow**: The code follows a logical sequence: 1. A new model is created and configured. 2. The model is trained using the small `manual_train_set`. 3. The trained model's performance is quantitatively evaluated using the `manual_test_set`. 4. The code enters an interactive loop, allowing the user to qualitatively test the model's behavior.\n",
        "\n",
        "*   **Comments and Observations**: For this next step, I used the spacy library to build and train a text classification model from scratch. I configured a blank English model, added the \"SPAM\" and \"HAM\" labels, and then trained it only on the 16 manually created examples from the first part. Recognizing that this was a very small dataset, I ran the training process for 10 epochs, hoping the repetition would help solidify the learning. After training, I evaluated its performance on the 4 unseen test samples, adding code from sklearn to calculate and print the accuracy and a more detailed classification report, which will show me how well it identifies each category. The most interesting part was the interactive testing loop at the end, which allowed me to input my own email text and see how this minimally-trained model would classify it in real-time, giving me a direct way to gauge its effectiveness."
      ],
      "metadata": {
        "id": "J8RaLdN0hte9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "from spacy.training import Example\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "import random\n",
        "\n",
        "# --- Model 1: Trained on MANUAL Data Only ---\n",
        "\n",
        "# --- PROVIDED SAMPLE CODE STARTS HERE (Initialization) ---\n",
        "# Load a blank model and add text classifier\n",
        "nlp_manual = spacy.blank(\"en\")\n",
        "textcat_manual = nlp_manual.add_pipe(\"textcat\")\n",
        "\n",
        "# Add labels for classification\n",
        "textcat_manual.add_label(\"SPAM\")\n",
        "textcat_manual.add_label(\"HAM\")\n",
        "\n",
        "# Training the model\n",
        "optimizer = nlp_manual.initialize()\n",
        "# --- PROVIDED SAMPLE CODE ENDS HERE (Initialization) ---\n",
        "\n",
        "print(\"--- Training Model 1 on 16 MANUAL examples ---\")\n",
        "# NOTE: The outer loop for epochs is an addition to improve performance.\n",
        "for i in range(10):\n",
        "    random.shuffle(manual_train_set)\n",
        "    # --- PROVIDED SAMPLE CODE STARTS HERE (Core Training Loop) ---\n",
        "    for text, annotations in manual_train_set:\n",
        "        example = Example.from_dict(nlp_manual.make_doc(text), annotations)\n",
        "        nlp_manual.update([example], sgd=optimizer)\n",
        "    # --- PROVIDED SAMPLE CODE ENDS HERE (Core Training Loop) ---\n",
        "print(\"Training of manual model complete.\")\n",
        "\n",
        "# --- Evaluation Metrics (ADDITION as per assignment requirements) ---\n",
        "print(\"\\n--- Evaluating Model 1 on 4 UNSEEN MANUAL examples ---\")\n",
        "true_labels = []\n",
        "predicted_labels = []\n",
        "for text, annotations in manual_test_set:\n",
        "    true_label = \"SPAM\" if annotations['cats']['SPAM'] == 1.0 else \"HAM\"\n",
        "    true_labels.append(true_label)\n",
        "    doc = nlp_manual(text) # Using the trained model to predict\n",
        "    predicted_label = \"SPAM\" if doc.cats['SPAM'] > doc.cats['HAM'] else \"HAM\"\n",
        "    predicted_labels.append(predicted_label)\n",
        "accuracy = accuracy_score(true_labels, predicted_labels)\n",
        "print(f\"\\nModel Accuracy on Manual Test Set: {accuracy * 100:.2f}%\")\n",
        "print(\"\\nClassification Report (Manual Model):\")\n",
        "print(classification_report(true_labels, predicted_labels, zero_division=0))\n",
        "# --- End of Evaluation Metrics Addition ---\n",
        "\n",
        "\n",
        "# --- PROVIDED SAMPLE CODE STARTS HERE (Interactive Testing) ---\n",
        "# Function to classify user input emails\n",
        "def classify_email_manual(email):\n",
        "    doc = nlp_manual(email)\n",
        "    spam_score = doc.cats['SPAM']\n",
        "    ham_score = doc.cats['HAM']\n",
        "    if spam_score > ham_score:\n",
        "        return \"SPAM\"\n",
        "    else:\n",
        "        return \"HAM\"\n",
        "\n",
        "# Allow users to test the model by inputting their own email data\n",
        "while True:\n",
        "    user_input = input(\"\\nTest the MANUAL Model: Enter an email (or type 'exit' to quit): \")\n",
        "    if user_input.lower() == 'exit':\n",
        "        break\n",
        "    classification = classify_email_manual(user_input)\n",
        "    print(f\"The email is classified as: {classification}\")\n",
        "# --- PROVIDED SAMPLE CODE ENDS HERE (Interactive Testing) ---"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jVh3E0rzhueA",
        "outputId": "aba510a3-4f83-4091-cf6f-5875412a8020"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- Training Model 1 on 16 MANUAL examples ---\n",
            "Training of manual model complete.\n",
            "\n",
            "--- Evaluating Model 1 on 4 UNSEEN MANUAL examples ---\n",
            "\n",
            "Model Accuracy on Manual Test Set: 75.00%\n",
            "\n",
            "Classification Report (Manual Model):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         HAM       1.00      0.67      0.80         3\n",
            "        SPAM       0.50      1.00      0.67         1\n",
            "\n",
            "    accuracy                           0.75         4\n",
            "   macro avg       0.75      0.83      0.73         4\n",
            "weighted avg       0.88      0.75      0.77         4\n",
            "\n",
            "\n",
            "Test the MANUAL Model: Enter an email (or type 'exit' to quit): YOU WON BIG TIME\n",
            "The email is classified as: SPAM\n",
            "\n",
            "Test the MANUAL Model: Enter an email (or type 'exit' to quit): Great\n",
            "The email is classified as: SPAM\n",
            "\n",
            "Test the MANUAL Model: Enter an email (or type 'exit' to quit): meeting tommorow\n",
            "The email is classified as: HAM\n",
            "\n",
            "Test the MANUAL Model: Enter an email (or type 'exit' to quit): exit\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.3: Model Trained on Sourced Data\n",
        "\n",
        "\n",
        "*   **Function Description**: This block mirrors the structure of Block 2 but operates on the large, sourced dataset. It initializes a second, completely independent spaCy model (`nlp_sourced`), trains it on the thousands of email samples in `sourced_train_set`, and evaluates it on the large `sourced_test_set`. It uses the same core training logic from the professor's sample code. The goal is to build a high-performance model and compare its results directly to the baseline model from Block 2.\n",
        "\n",
        "*   **Syntax Explanation**:\n",
        "    *   **`# --- PROVIDED SAMPLE CODE ...`**: As in the previous block, these comments highlight the use of the professor's core implementation for initialization, training, and interactive testing.\n",
        "    *   `nlp_sourced = spacy.blank(\"en\")`: Creates a *new, separate* model object to ensure it does not have any \"memory\" of the manual data.\n",
        "    *   `nlp_sourced.update(...)`: The exact same training method as before, but this time it is being fed thousands of examples from the `sourced_train_set`.\n",
        "    *   `classification_report(...)`: The same scikit-learn function is used here to provide a robust evaluation of this more powerful model.\n",
        "    *   `classify_email_sourced(email)`: A distinct classification function for the interactive loop to ensure we are testing the correct model (`nlp_sourced`).\n",
        "\n",
        "*   **Inputs**:\n",
        "    1.  `sourced_train_set`: The large list of training examples (80% of `emails.csv`) from Block 1.\n",
        "    2.  `sourced_test_set`: The large list of testing examples (20% of `emails.csv`) from Block 1.\n",
        "    3.  `user_input`: Text entered by the user during this block's interactive testing phase.\n",
        "\n",
        "*   **Outputs**:\n",
        "    1.  A printed log showing the training process on the large dataset.\n",
        "    2.  A final accuracy score and Classification Report detailing the high performance of the model on the sourced test set.\n",
        "    3.  A final interactive prompt for testing this superior model.\n",
        "\n",
        "*   **Code Flow**: The flow is identical to Block 2, emphasizing the experimental nature: 1. A new model is created. 2. The model is trained, but this time on the large `sourced_train_set`. 3. The model is evaluated on the large `sourced_test_set`. 4. An interactive loop is started for this specific model.\n",
        "\n",
        "*   **Comments and Observations**: In this cell, I set up a second, completely separate spacy model to see the impact of using a much larger dataset. This time, I'm training the model exclusively on the thousands of email examples I loaded from the CSV file. A key change I made here was to the training loop; because there are so many examples, updating the model one-by-one would be incredibly slow, so I implemented batching to process 32 emails at a time, which makes the training process much more efficient. After running the training for 10 epochs, I evaluated this new model against its corresponding large test set, and I anticipate the accuracy will be significantly higher than the first model's. The final interactive prompt is great because it will let me directly compare the classification ability of this well-trained model against the previous one that was only trained on 16 examples."
      ],
      "metadata": {
        "id": "Tc7s5MfJh2Xe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "from spacy.training import Example\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "import random\n",
        "\n",
        "# --- Model 2: Trained on SOURCED Data Only ---\n",
        "\n",
        "# --- PROVIDED SAMPLE CODE STARTS HERE (Initialization) ---\n",
        "nlp_sourced = spacy.blank(\"en\")\n",
        "textcat_sourced = nlp_sourced.add_pipe(\"textcat\")\n",
        "textcat_sourced.add_label(\"SPAM\")\n",
        "textcat_sourced.add_label(\"HAM\")\n",
        "optimizer = nlp_sourced.initialize()\n",
        "# --- PROVIDED SAMPLE CODE ENDS HERE (Initialization) ---\n",
        "\n",
        "print(f\"--- Training Model 2 on {len(sourced_train_set)} SOURCED examples (using efficient batching) ---\")\n",
        "\n",
        "# --- MODIFICATION FOR SPEED: BATCHING ---\n",
        "# This loop structure is an optimization to handle the large dataset efficiently.\n",
        "# It does not change the core logic of the professor's nlp.update() function.\n",
        "n_epochs = 10\n",
        "batch_size = 32\n",
        "for i in range(n_epochs):\n",
        "    random.shuffle(sourced_train_set)\n",
        "    # Use spaCy's minibatch utility to create batches\n",
        "    batches = spacy.util.minibatch(sourced_train_set, size=batch_size)\n",
        "    for batch in batches:\n",
        "        # Create Example objects for the whole batch\n",
        "        examples = []\n",
        "        for text, annotations in batch:\n",
        "            examples.append(Example.from_dict(nlp_sourced.make_doc(text), annotations))\n",
        "\n",
        "        # --- PROVIDED SAMPLE CODE STARTS HERE (Core Update Call) ---\n",
        "        # Update the model with the batch of examples\n",
        "        nlp_sourced.update(examples, sgd=optimizer)\n",
        "        # --- PROVIDED SAMPLE CODE ENDS HERE (Core Update Call) ---\n",
        "    print(f\"Completed Epoch {i+1}/{n_epochs}\")\n",
        "# --- END OF BATCHING MODIFICATION ---\n",
        "\n",
        "print(\"Training of sourced model complete.\")\n",
        "\n",
        "# --- Evaluation Metrics ---\n",
        "print(f\"\\n--- Evaluating Model 2 on {len(sourced_test_set)} UNSEEN SOURCED examples ---\")\n",
        "true_labels = []\n",
        "predicted_labels = []\n",
        "for text, annotations in sourced_test_set:\n",
        "    true_label = \"SPAM\" if annotations['cats']['SPAM'] == 1.0 else \"HAM\"\n",
        "    true_labels.append(true_label)\n",
        "    doc = nlp_sourced(text)\n",
        "    predicted_label = \"SPAM\" if doc.cats['SPAM'] > doc.cats['HAM'] else \"HAM\"\n",
        "    predicted_labels.append(predicted_label)\n",
        "accuracy = accuracy_score(true_labels, predicted_labels)\n",
        "print(f\"\\nModel Accuracy on Sourced Test Set: {accuracy * 100:.2f}%\")\n",
        "print(\"\\nClassification Report (Sourced Model):\")\n",
        "print(classification_report(true_labels, predicted_labels, zero_division=0))\n",
        "# --- End of Evaluation Metrics ---\n",
        "\n",
        "\n",
        "# --- PROVIDED SAMPLE CODE STARTS HERE (Interactive Testing) ---\n",
        "def classify_email_sourced(email):\n",
        "    doc = nlp_sourced(email)\n",
        "    spam_score = doc.cats['SPAM']\n",
        "    ham_score = doc.cats['HAM']\n",
        "    if spam_score > ham_score:\n",
        "        return \"SPAM\"\n",
        "    else:\n",
        "        return \"HAM\"\n",
        "\n",
        "while True:\n",
        "    user_input = input(\"\\nTest the SOURCED Model: Enter an email (or type 'exit' to quit): \")\n",
        "    if user_input.lower() == 'exit':\n",
        "        break\n",
        "    classification = classify_email_sourced(user_input)\n",
        "    print(f\"The email is classified as: {classification}\")\n",
        "# --- PROVIDED SAMPLE CODE ENDS HERE (Interactive Testing) ---"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4SiGeJUPh22t",
        "outputId": "e1cefc43-a8f0-4485-e9a5-c80a24df3bf8"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- Training Model 2 on 4582 SOURCED examples (using efficient batching) ---\n",
            "Completed Epoch 1/10\n",
            "Completed Epoch 2/10\n",
            "Completed Epoch 3/10\n",
            "Completed Epoch 4/10\n",
            "Completed Epoch 5/10\n",
            "Completed Epoch 6/10\n",
            "Completed Epoch 7/10\n",
            "Completed Epoch 8/10\n",
            "Completed Epoch 9/10\n",
            "Completed Epoch 10/10\n",
            "Training of sourced model complete.\n",
            "\n",
            "--- Evaluating Model 2 on 1146 UNSEEN SOURCED examples ---\n",
            "\n",
            "Model Accuracy on Sourced Test Set: 98.08%\n",
            "\n",
            "Classification Report (Sourced Model):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         HAM       0.98      1.00      0.99       874\n",
            "        SPAM       0.99      0.93      0.96       272\n",
            "\n",
            "    accuracy                           0.98      1146\n",
            "   macro avg       0.98      0.96      0.97      1146\n",
            "weighted avg       0.98      0.98      0.98      1146\n",
            "\n",
            "\n",
            "Test the SOURCED Model: Enter an email (or type 'exit' to quit): Hey. We have a meetingn tommorow\n",
            "The email is classified as: HAM\n",
            "\n",
            "Test the SOURCED Model: Enter an email (or type 'exit' to quit): Subject: re : houston visit  great ! i look forward to our dinner on thurs . 12 / 7 evening . hopefully your  flight will be on time . . . although having watched 60 minutes last night and  suffered from a # of delays lately , let ' s hope that the \" weather blame \"  doesn ' t get in the way . it ' s best to leave me a message @ my usual work #  on thurs . , 914 253 4187 , . . . i can easily check it in houston .  i ' ll be staying @ the westin oaks in the galleria . . . any preferred place  that i can book ( & for what time ) ? ? coming over to down town won ' t be a  problem for me either .  will be great to see you again .  soussan  914 253 4187  - - - - - original message - - - - -  from : vince . j . kaminski @ enron . com [ mailto : vince . j . kaminski @ enron . com ]  sent : monday , november 27 , 2000 12 : 10 pm  to : faizs @ texaco . com  cc : vince . j . kaminski @ enron . com  subject : re : houston visit  soussan ,  thanks for your message . it would be great to meet you when you come to  houston .  i shall be in town on december 7 , flying back from philly in the morning .  assuming that the flight is on schedule , i shall be available for dinner .  please , let me know how i can contact you on thursday , december the 7 th ,  to confirm .  look forward to meeting you .  vince  \" faiz , soussan \" on 11 / 26 / 2000 09 : 04 : 01 pm  to : \" ' vince . j . kaminski @ enron . com ' \"  cc :  subject : houston visit  dear vince ,  greetings from ny and hope that all is well and you had a great  thanksgiving . i ' ll be coming to houston for 12 / 6 - 12 / 7 and hope you are  available either evening for dinner . would be great to see you again and  catch up with the latest . . . i really enjoyed my visit last april , your  insights , and the risk book you gave me .  i do hope you ' re available to meet and pls let me know which evening suits  you better .  best ,  soussan faiz  texaco inc .  914 253 4187\n",
            "The email is classified as: HAM\n",
            "\n",
            "Test the SOURCED Model: Enter an email (or type 'exit' to quit): Subject: undelivered mail returned to sender  this is the postfix program at host mx 2 . oi . com . br .  i ' m sorry to have to inform you that your message could not be  be delivered to one or more recipients . it ' s attached below .  for further assistance , please send mail to  if you do so , please include this problem report . you can  delete your own text from the attached returned message .  the postfix program  : host frontend . oi . com . br [ 200 . 222 . 115 . 18 ] said : 550 - mailbox  unknown . either there is no mailbox associated with this 550 - name or you  do not have authorization to see it . 550 5 . 1 . 1 user unknown ( in reply to  end of data command )\n",
            "The email is classified as: SPAM\n",
            "\n",
            "Test the SOURCED Model: Enter an email (or type 'exit' to quit): You won big time\n",
            "The email is classified as: HAM\n",
            "\n",
            "Test the SOURCED Model: Enter an email (or type 'exit' to quit): exit\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 2: Part-of-Speech (POS) Tagging\n",
        "\n"
      ],
      "metadata": {
        "id": "TYYlx4m2HeM4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Objective:**\n",
        "This section implements a Part-of-Speech (POS) tagger using a pre-trained spaCy model. The goal is to take a sentence, process it, and identify the grammatical category for each word (e.g., noun, verb, adjective). This task demonstrates the use of pre-built NLP pipelines for linguistic feature extraction and introduces methods for evaluating the model's performance.\n",
        "\n",
        "---\n",
        "\n",
        "### 2.1. POS Tagger Implementation\n",
        "\n",
        "**Code Cell Description:**\n",
        "This first code block loads a pre-trained spaCy model and uses it to perform POS tagging on a text string provided by the user. Unlike a task that requires training, this process leverages a model that has already learned the patterns of the English language.\n",
        "\n",
        "*   **Function Description:**\n",
        "    The `analyze_text(text)` function serves as the core of this tool. It accepts a string of text, processes it using the loaded spaCy pipeline, and extracts the POS tag for each token in the text.\n",
        "\n",
        "*   **Syntax Explanation:**\n",
        "    *   `nlp = spacy.load(\"en_core_web_sm\")`: This command loads a small, pre-trained English language model provided by spaCy. \"Pre-trained\" means the model has already been trained by spaCy's developers on a massive corpus of text. This allows us to use it for tasks like POS tagging out-of-the-box.\n",
        "    *   `doc = nlp(text)`: The input text is passed to the `nlp` object. This returns a `doc` object, which is a rich, processed container holding the original text along with all the linguistic annotations (like tokens, POS tags, dependencies, etc.) discovered by the model.\n",
        "    *   `[(token.text, token.pos_) for token in doc]`: This is a list comprehension, a concise way to create a list. It iterates through every token in the `doc` object and, for each token, creates a tuple containing the original word (`token.text`) and its universal POS tag (`token.pos_`).\n",
        "\n",
        "*   **Inputs:**\n",
        "    *   The primary input is a text string (`user_input`) provided by the user via the keyboard prompt.\n",
        "    *   The implicit input is the `en_core_web_sm` model, which contains all the necessary data and weights to perform the analysis.\n",
        "\n",
        "*   **Outputs:**\n",
        "    *   The code prints a list of tuples to the console.\n",
        "    *   Each tuple represents a word from the input sentence and its corresponding POS tag. For example, the input \"She is reading a book.\" would produce the output `[('She', 'PRON'), ('is', 'AUX'), ('reading', 'VERB'), ('a', 'DET'), ('book', 'NOUN'), ('.', 'PUNCT')]`.\n",
        "\n",
        "*   **Code Flow:**\n",
        "    1.  The `spacy.load()` command initializes the pre-trained NLP pipeline.\n",
        "    2.  The program prompts the user to enter a sentence.\n",
        "    3.  The user's input is passed to the `analyze_text` function.\n",
        "    4.  Inside the function, the text is processed by the `nlp` pipeline into a `doc` object.\n",
        "    5.  A list of (token, POS tag) tuples is generated and returned.\n",
        "    6.  The program prints the final list to the console.\n",
        "\n",
        "*   **Comments and Observations:**\n",
        "    This task highlights the power of transfer learning in NLP. By leveraging a pre-trained model, we can perform complex linguistic analysis with very little code and without the need for a custom dataset or a lengthy training process. POS tagging is a foundational NLP task that is often a preprocessing step for more complex applications like Named Entity Recognition (NER), information extraction, and sentiment analysis.\n",
        "\n",
        "---\n",
        "\n",
        "### Model Evaluation and Performance Metrics\n",
        "\n",
        "**Code Cell Description:**\n",
        "This second code block is dedicated to evaluating the performance of our spaCy POS tagging model. To assess its accuracy, we compare its predictions against a manually-labeled \"gold standard\" test dataset. We then calculate several standard classification metrics using the `scikit-learn` library to quantify its performance.\n",
        "\n",
        "*   **Code Flow:**\n",
        "    1.  **Create a Ground Truth Dataset**: A list named `ground_truth_data` is defined. Each item in the list is a tuple containing a sentence and a corresponding list of `(word, correct_POS_tag)` tuples. This serves as our testing data.\n",
        "    2.  **Generate Model Predictions**: The code loops through the ground truth dataset. For each sentence, it uses our `analyze_text` function to get the model's predicted POS tags.\n",
        "    3.  **Prepare Data for Evaluation**: Two flat lists are created: `all_true_tags` (from our ground truth data) and `all_predicted_tags` (from the model). Flattening the lists is necessary to use the `scikit-learn` metrics functions.\n",
        "    4.  **Calculate Metrics**: Using the prepared lists, the code calculates Accuracy, Precision, Recall, and the F1-Score.\n",
        "    5.  **Display Results**: The calculated metrics and a detailed `classification_report` are printed to show the model's performance, both overall and for each specific POS tag.\n",
        "\n",
        "*   **Inputs:**\n",
        "    *   `ground_truth_data`: The manually created test set containing sentences and their correct POS tags.\n",
        "    *   `all_true_tags`: A Python list containing the correct, manually-verified POS tags from our ground truth dataset. This is derived from `ground_truth_data`.\n",
        "    *   `all_predicted_tags`: A Python list containing the POS tags predicted by the spaCy model for the same set of text.\n",
        "\n",
        "*   **Outputs:**\n",
        "    *   A printout of four key performance scores (Accuracy, Precision, Recall, F1-Score).\n",
        "    *   A detailed `Classification Report` that breaks down the performance for each individual POS tag (e.g., 'NOUN', 'VERB', 'ADJ'), showing its precision, recall, and F1-score.\n",
        "\n",
        "---\n",
        "\n",
        "### Explanation of Chosen Metrics\n",
        "\n",
        "#### 1. **Accuracy**\n",
        "*   **What it is:** Accuracy is the most intuitive metric. It measures the proportion of tokens that were tagged correctly out of all the tokens.\n",
        "*   **Formula:** `Accuracy = (Number of Correctly Predicted Tags) / (Total Number of Tags)`\n",
        "*   **Why it's useful:** It gives a quick, overall summary of the model's performance. An accuracy of 0.95 means that 95% of the words in the test set were assigned the correct POS tag. However, it can be misleading if the dataset is imbalanced.\n",
        "\n",
        "#### 2. **Precision**\n",
        "*   **What it is:** Precision answers the question: \"Of all the tokens that the model labeled as a specific tag (e.g., NOUN), how many were actually NOUNs?\"\n",
        "*   **Formula:** `Precision = (True Positives) / (True Positives + False Positives)`\n",
        "*   **Why it's useful:** High precision for a specific tag means that when the model identifies that tag, it is very likely to be correct. It is a measure of a classifier's exactness and helps gauge the rate of false positives.\n",
        "\n",
        "#### 3. **Recall**\n",
        "*   **What it is:** Recall answers the question: \"Of all the tokens that were actually a specific tag (e.g., NOUN), how many did the model correctly identify?\"\n",
        "*   **Formula:** `Recall = (True Positives) / (True Positives + False Negatives)`\n",
        "*   **Why it's useful:** High recall for a specific tag means that the model is good at finding all instances of that tag in the text. It is a measure of a classifier's completeness and helps gauge the rate of false negatives.\n",
        "\n",
        "#### 4. **F1-Score**\n",
        "*   **What it is:** The F1-Score is the harmonic mean of Precision and Recall. It provides a single score that balances both concerns.\n",
        "*   **Formula:** `F1-Score = 2 * (Precision * Recall) / (Precision + Recall)`\n",
        "*   **Why it's useful:** It is a robust metric, especially when there's an uneven class distribution (some tags are much more common than others). A high F1-score indicates that the model has both low false positives and low false negatives.\n",
        "\n",
        "**Note on \"Weighted\" Average:**\n",
        "We use the `average='weighted'` parameter when calculating precision, recall, and F1-score. This is crucial because our dataset has an imbalanced number of tags (e.g., more NOUNs than SYMbols). This method calculates the metrics for each tag independently and then computes an average, weighted by the number of instances of each tag in the true data. This provides a more fair and accurate overall picture of performance than a simple average.\n",
        "\n",
        "\n",
        "Comments and observations: I loaded one of spaCy's pre-trained English models, en_core_web_sm, to evaluate its built-in capability for Part-of-Speech (POS) tagging. My main objective here was to measure the performance of this professional-grade model. To do this, I created a small \"gold standard\" dataset by manually labeling the correct grammatical tags for a few sentences. I then wrote code to get the model's predictions for these same sentences and compared them against my ground truth labels. The core of this exercise was using sklearn.metrics to calculate not just a simple accuracy score, but also the weighted precision, recall, and F1-score, which provide a more nuanced view of performance, especially if some tags are more common than others. The detailed classification report at the end was the most valuable part, as it broke down the model's performance for every single POS tag, showing exactly where it's strong and where it might be less reliable."
      ],
      "metadata": {
        "id": "exj5QLraJuJh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
        "\n",
        "# Load a pre-trained English spaCy model.\n",
        "# 'en_core_web_sm' is a small, efficient model for English.\n",
        "try:\n",
        "    nlp = spacy.load(\"en_core_web_sm\")\n",
        "except OSError:\n",
        "    print('Downloading language model for the first time. This may take a few minutes...')\n",
        "    from spacy.cli import download\n",
        "    download('en_core_web_sm')\n",
        "    nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# --- START OF PROVIDED SAMPLE CODE ---\n",
        "\n",
        "# Function to analyze user input text and return tokens with POS tags as a list\n",
        "def analyze_text(text):\n",
        "    # Process the text with the spaCy pipeline\n",
        "    doc = nlp(text)\n",
        "\n",
        "    # Create a list of tuples, where each tuple contains the token's text and its POS tag\n",
        "    pos_list = [(token.text, token.pos_) for token in doc]\n",
        "    return pos_list\n",
        "\n",
        "# Allow user input and analyze\n",
        "user_input = input(\"Enter a text for POS tagging analysis: \")\n",
        "pos_tags = analyze_text(user_input)\n",
        "\n",
        "# Display the result as a list\n",
        "print(\"\\nTokens and POS Tags:\")\n",
        "print(pos_tags)\n",
        "\n",
        "# --- END OF PROVIDED SAMPLE CODE ---\n",
        "\n",
        "\n",
        "# --- START OF METRICS IMPLEMENTATION ---\n",
        "\n",
        "# Step 1: Create a manually labeled \"gold standard\" dataset for evaluation.\n",
        "# This dataset contains sentences and their correctly hand-labeled POS tags.\n",
        "# NOTE: The tagset used (e.g., 'PROPN', 'VERB') must match spaCy's Universal Dependencies tagset.\n",
        "ground_truth_data = [\n",
        "    (\"Apple is looking at buying U.K. startup for $1 billion.\",\n",
        "     [('Apple', 'PROPN'), ('is', 'AUX'), ('looking', 'VERB'), ('at', 'ADP'), ('buying', 'VERB'), ('U.K.', 'PROPN'), ('startup', 'NOUN'), ('for', 'ADP'), ('$', 'SYM'), ('1', 'NUM'), ('billion', 'NUM'), ('.', 'PUNCT')]),\n",
        "    (\"The quick brown fox jumps over the lazy dog.\",\n",
        "     [('The', 'DET'), ('quick', 'ADJ'), ('brown', 'ADJ'), ('fox', 'NOUN'), ('jumps', 'VERB'), ('over', 'ADP'), ('the', 'DET'), ('lazy', 'ADJ'), ('dog', 'NOUN'), ('.', 'PUNCT')]),\n",
        "    (\"I love to write code in Python because it is versatile.\",\n",
        "     [('I', 'PRON'), ('love', 'VERB'), ('to', 'PART'), ('write', 'VERB'), ('code', 'NOUN'), ('in', 'ADP'), ('Python', 'PROPN'), ('because', 'SCONJ'), ('it', 'PRON'), ('is', 'AUX'), ('versatile', 'ADJ'), ('.', 'PUNCT')]),\n",
        "    (\"She quickly reads the book.\",\n",
        "     [('She', 'PRON'), ('quickly', 'ADV'), ('reads', 'VERB'), ('the', 'DET'), ('book', 'NOUN'), ('.', 'PUNCT')]),\n",
        "]\n",
        "\n",
        "# Step 2: Get model's predictions and flatten lists for comparison\n",
        "all_true_tags = []\n",
        "all_predicted_tags = []\n",
        "\n",
        "print(\"\\n--- Model Performance Evaluation ---\")\n",
        "for sentence, true_tags in ground_truth_data:\n",
        "    # Extract just the tags from the ground truth data\n",
        "    true_pos = [tag for word, tag in true_tags]\n",
        "    all_true_tags.extend(true_pos)\n",
        "\n",
        "    # Get predictions from our spaCy model\n",
        "    predicted_pos_tuples = analyze_text(sentence)\n",
        "    predicted_pos = [tag for word, tag in predicted_pos_tuples]\n",
        "    all_predicted_tags.extend(predicted_pos)\n",
        "\n",
        "    print(f\"\\nSentence: '{sentence}'\")\n",
        "    print(f\"  > True Tags:     {true_pos}\")\n",
        "    print(f\"  > Predicted Tags: {predicted_pos}\")\n",
        "\n",
        "\n",
        "# Step 3: Calculate Performance Metrics\n",
        "# Ensure that we have the same number of tags to compare\n",
        "if len(all_true_tags) == len(all_predicted_tags):\n",
        "    # Overall Accuracy\n",
        "    accuracy = accuracy_score(all_true_tags, all_predicted_tags)\n",
        "\n",
        "    # Precision, Recall, and F1-Score\n",
        "    # 'weighted' average calculates metrics for each label, and finds their average,\n",
        "    # weighted by the number of true instances for each label. This accounts for label imbalance.\n",
        "    precision = precision_score(all_true_tags, all_predicted_tags, average='weighted', zero_division=0)\n",
        "    recall = recall_score(all_true_tags, all_predicted_tags, average='weighted', zero_division=0)\n",
        "    f1 = f1_score(all_true_tags, all_predicted_tags, average='weighted', zero_division=0)\n",
        "\n",
        "    print(\"\\n--- Overall Model Metrics ---\")\n",
        "    print(f\"Accuracy: {accuracy:.4f}\")\n",
        "    print(f\"Weighted Precision: {precision:.4f}\")\n",
        "    print(f\"Weighted Recall: {recall:.4f}\")\n",
        "    print(f\"Weighted F1-Score: {f1:.4f}\")\n",
        "\n",
        "    # Detailed report showing metrics for each POS tag\n",
        "    print(\"\\n--- Classification Report (Metrics Per POS Tag) ---\")\n",
        "    # Get the unique labels from both true and predicted lists to include all tags in the report\n",
        "    labels = sorted(list(set(all_true_tags + all_predicted_tags)))\n",
        "    print(classification_report(all_true_tags, all_predicted_tags, labels=labels, zero_division=0))\n",
        "else:\n",
        "    print(\"\\nError: Mismatch between the number of true and predicted tags. Cannot calculate metrics.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9t5XfQ8FHezd",
        "outputId": "221a240b-ffa6-4519-d0be-15507a5dd25b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter a text for POS tagging analysis: The quick brown fox jumps over the lazy dog\n",
            "\n",
            "Tokens and POS Tags:\n",
            "[('The', 'DET'), ('quick', 'ADJ'), ('brown', 'ADJ'), ('fox', 'NOUN'), ('jumps', 'VERB'), ('over', 'ADP'), ('the', 'DET'), ('lazy', 'ADJ'), ('dog', 'NOUN')]\n",
            "\n",
            "--- Model Performance Evaluation ---\n",
            "\n",
            "Sentence: 'Apple is looking at buying U.K. startup for $1 billion.'\n",
            "  > True Tags:     ['PROPN', 'AUX', 'VERB', 'ADP', 'VERB', 'PROPN', 'NOUN', 'ADP', 'SYM', 'NUM', 'NUM', 'PUNCT']\n",
            "  > Predicted Tags: ['PROPN', 'AUX', 'VERB', 'ADP', 'VERB', 'PROPN', 'VERB', 'ADP', 'SYM', 'NUM', 'NUM', 'PUNCT']\n",
            "\n",
            "Sentence: 'The quick brown fox jumps over the lazy dog.'\n",
            "  > True Tags:     ['DET', 'ADJ', 'ADJ', 'NOUN', 'VERB', 'ADP', 'DET', 'ADJ', 'NOUN', 'PUNCT']\n",
            "  > Predicted Tags: ['DET', 'ADJ', 'ADJ', 'NOUN', 'VERB', 'ADP', 'DET', 'ADJ', 'NOUN', 'PUNCT']\n",
            "\n",
            "Sentence: 'I love to write code in Python because it is versatile.'\n",
            "  > True Tags:     ['PRON', 'VERB', 'PART', 'VERB', 'NOUN', 'ADP', 'PROPN', 'SCONJ', 'PRON', 'AUX', 'ADJ', 'PUNCT']\n",
            "  > Predicted Tags: ['PRON', 'VERB', 'PART', 'VERB', 'NOUN', 'ADP', 'PROPN', 'SCONJ', 'PRON', 'AUX', 'ADJ', 'PUNCT']\n",
            "\n",
            "Sentence: 'She quickly reads the book.'\n",
            "  > True Tags:     ['PRON', 'ADV', 'VERB', 'DET', 'NOUN', 'PUNCT']\n",
            "  > Predicted Tags: ['PRON', 'ADV', 'VERB', 'DET', 'NOUN', 'PUNCT']\n",
            "\n",
            "--- Overall Model Metrics ---\n",
            "Accuracy: 0.9750\n",
            "Weighted Precision: 0.9786\n",
            "Weighted Recall: 0.9750\n",
            "Weighted F1-Score: 0.9746\n",
            "\n",
            "--- Classification Report (Metrics Per POS Tag) ---\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         ADJ       1.00      1.00      1.00         4\n",
            "         ADP       1.00      1.00      1.00         4\n",
            "         ADV       1.00      1.00      1.00         1\n",
            "         AUX       1.00      1.00      1.00         2\n",
            "         DET       1.00      1.00      1.00         3\n",
            "        NOUN       1.00      0.80      0.89         5\n",
            "         NUM       1.00      1.00      1.00         2\n",
            "        PART       1.00      1.00      1.00         1\n",
            "        PRON       1.00      1.00      1.00         3\n",
            "       PROPN       1.00      1.00      1.00         3\n",
            "       PUNCT       1.00      1.00      1.00         4\n",
            "       SCONJ       1.00      1.00      1.00         1\n",
            "         SYM       1.00      1.00      1.00         1\n",
            "        VERB       0.86      1.00      0.92         6\n",
            "\n",
            "    accuracy                           0.97        40\n",
            "   macro avg       0.99      0.99      0.99        40\n",
            "weighted avg       0.98      0.97      0.97        40\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Part 3: Sentiment Analysis\n",
        "\n"
      ],
      "metadata": {
        "id": "uKD2vFYJ3-lf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.1. Data Loading and Preparation for Sentiment Analysis\n",
        "\n",
        "**Objective:** To load the raw IMDb review data and prepare it for model training and evaluation by splitting it into distinct training and testing sets.\n",
        "\n",
        "**Dataset:** The IMDb Large Movie Review Dataset is used for this task. It contains 50,000 movie reviews, each labeled with a corresponding 'positive' or 'negative' sentiment.\n",
        "\n",
        "**Methodology:**\n",
        "\n",
        "The project requires a robust evaluation of the model's performance on unseen data to ensure the reported accuracy is a true measure of its capabilities. Since the dataset was provided as a single file (`IMDB Dataset.csv`), it was manually divided into two subsets:\n",
        "\n",
        "1.  **Training Set (80% of the data):** This portion is used exclusively to train the model. The model learns the patterns, vocabulary, and sentence structures associated with positive and negative reviews from this data.\n",
        "2.  **Testing Set (20% of the data):** This portion is held back and is **never shown to the model during training**. It serves as a final, unbiased exam to evaluate the model's ability to generalize to new, unseen reviews.\n",
        "\n",
        "The split was performed using the `train_test_split` function from the `scikit-learn` library with the following critical parameters:\n",
        "*   `test_size=0.2`: This parameter allocates 20% of the total data to the testing set, leaving 80% for training, which is a standard practice in machine learning.\n",
        "*   `random_state=42`: This ensures the split is **reproducible**. Anyone running this notebook will get the exact same training and testing sets, ensuring the results can be verified.\n",
        "*   `stratify=full_df['label']`: This is a crucial step that ensures the proportion of positive and negative reviews is identical in both the training and testing sets. This prevents an imbalanced split that could bias the model's training or its evaluation.\n",
        "\n",
        "**Label Encoding:**\n",
        "The original sentiment labels ('positive', 'negative') were converted into numerical format (`1` for positive, `0` for negative) as machine learning models require numeric inputs.\n",
        "\n",
        "**Reference:**\n",
        "- The dataset is a processed version of the one introduced by Maas, A. L., et al. (2011) in *Learning Word Vectors for Sentiment Analysis*.\n",
        "- The `IMDB Dataset.csv` file was loaded from Google Drive on September 10, 2025.\n",
        "\n",
        "**Comments and observation:** This cell is all about setting up the data for a new sentiment analysis model. I had initially did try to use a manual dataset made by me but the accuracy was very low no matter how large of a data set I did. I would need to create a very large dataset with a very good quality data to have a good accuracy but that would take me too much. I could copy and paste real sentiment reviews or paraphrase them but that would be acadmically dishonest so I did not do so. So I went and used a sourced dataset to see its performance. The first thing I had to do was mount my Google Drive to get access to the IMDB Dataset.csv. I loaded it using pandas and included a try-except block, which is a good practice to make sure my file path is correct before the program crashes. Once the data was loaded, I performed a crucial preprocessing step: converting the text labels 'positive' and 'negative' into numerical values, 1 and 0, which is what machine learning models typically need. The final and most important step was using train_test_split from scikit-learn to divide the entire dataset into a training set (80%) and a testing set (20%), and I made sure to use the stratify option to keep the proportion of positive and negative reviews equal in both splits, which prevents any bias in the evaluation later on."
      ],
      "metadata": {
        "id": "NGXOsxaFSIww"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7iPEHzEI0dag",
        "outputId": "a0cba2ad-a064-4cf1-a368-2f2cc04f27a4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Dataset loaded successfully!\n",
            "Total reviews: 50000\n",
            "                                              review sentiment\n",
            "0  One of the other reviewers has mentioned that ...  positive\n",
            "1  A wonderful little production. <br /><br />The...  positive\n",
            "2  I thought this was a wonderful way to spend ti...  positive\n",
            "3  Basically there's a family where a little boy ...  negative\n",
            "4  Petter Mattei's \"Love in the Time of Money\" is...  positive\n",
            "\n",
            "Training set size: 40000\n",
            "Testing set size: 10000\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from google.colab import drive\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# --- 1. Mount Google Drive ---\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# --- 2. Define the file path ---\n",
        "# Use the path you copied from the file browser. This is the most likely path.\n",
        "file_path = '/content/IMDB Dataset/IMDB Dataset.csv'\n",
        "\n",
        "# --- 3. Load the dataset ---\n",
        "try:\n",
        "    full_df = pd.read_csv(file_path)\n",
        "    print(\"Dataset loaded successfully!\")\n",
        "    print(f\"Total reviews: {len(full_df)}\")\n",
        "    print(full_df.head()) # Display the first few rows\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: File not found at the path: {file_path}\")\n",
        "    print(\"Please double-check the path by right-clicking the file in the Colab file browser and selecting 'Copy path'.\")\n",
        "\n",
        "# --- 4. (From previous step) Preprocess and split the data ---\n",
        "# This part assumes the loading was successful.\n",
        "\n",
        "# Map text labels to numbers\n",
        "full_df['label'] = full_df['sentiment'].map({'positive': 1, 'negative': 0})\n",
        "\n",
        "# Split into training and testing sets\n",
        "train_df, test_df = train_test_split(\n",
        "    full_df,\n",
        "    test_size=0.2,\n",
        "    random_state=42,\n",
        "    stratify=full_df['label']\n",
        ")\n",
        "\n",
        "# Verify the split\n",
        "print(f\"\\nTraining set size: {len(train_df)}\")\n",
        "print(f\"Testing set size: {len(test_df)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.2. Data Formatting for spaCy\n",
        "\n",
        "**Code Cell Description:** This code cell is responsible for a critical data transformation step. It converts the data from the `train_df` and `test_df` pandas DataFrames into the specific list-of-tuples format required by spaCy's training loop. This acts as a bridge between our raw data and the machine learning model.\n",
        "\n",
        "*   **Function Description:**\n",
        "    The `format_data_for_spacy(df)` function iterates through a DataFrame, taking the 'review' text and its corresponding numeric 'label' (0 or 1) and converting them into a tuple containing the text and a structured dictionary for the annotations.\n",
        "\n",
        "*   **Syntax Explanation:**\n",
        "    *   `for index, row in df.iterrows()`: This is a standard pandas method to loop through each row of a DataFrame.\n",
        "    *   `annotations = {\"cats\": {\"POSITIVE\": 1.0, \"NEGATIVE\": 0.0}}`: This creates the dictionary in the specific \"gold-standard\" format that spaCy's `textcat` component expects for training. The key `\"cats\"` refers to the categories. The nested dictionary specifies the probability of each label for this text. For training, the correct label is given a probability of 1.0 and all others are 0.0.\n",
        "\n",
        "*   **Inputs:**\n",
        "    *   The function takes a pandas DataFrame (`df`) as input.\n",
        "    *   This DataFrame is expected to have a `review` column containing the text and a `label` column containing the numeric sentiment (1 for positive, 0 for negative).\n",
        "    *   The function is called twice: once with `train_df` and once with `test_df`.\n",
        "\n",
        "*   **Outputs:**\n",
        "    *   The function returns a Python list (e.g., `train_data_spacy`).\n",
        "    *   Each element in the list is a tuple `(text, annotations)`, where `text` is the movie review string and `annotations` is the structured dictionary.\n",
        "\n",
        "*   **Code Flow:**\n",
        "    The code first defines the conversion function. It then calls this function on the training DataFrame to create `train_data_spacy` and on the testing DataFrame to create `test_data_spacy`. This ensures that both datasets are in the correct format for their respective roles in training and evaluation.\n",
        "\n",
        "*  Comments and Obeservation:\n",
        "  This cell is a pure data transformation step, and it's a really important one. I learned from the previous spam filter exercise that spaCy's training process needs data in a very specific tuple format: (text, annotations). So, here I've created a reusable function to convert my pandas DataFrames into that required structure. The function iterates through every movie review, checks if its label is 1 (positive) or 0 (negative), and then builds the corresponding {\"cats\": {\"POSITIVE\": 1.0, \"NEGATIVE\": 0.0}} dictionary. I then applied this function to both the training and testing sets I created earlier. By printing a sample at the end, I could quickly verify that the conversion worked correctly, ensuring my data is now perfectly formatted and ready for the spaCy model training pipeline in the next step."
      ],
      "metadata": {
        "id": "V-wKqYQtAY3U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Format the IMDb data for spaCy\n",
        "\n",
        "print(\"Formatting data for spaCy...\")\n",
        "\n",
        "# The spaCy model needs the data in a specific format.\n",
        "# We'll create a function to convert our DataFrame rows into this format.\n",
        "def format_data_for_spacy(df):\n",
        "    formatted_data = []\n",
        "    for index, row in df.iterrows():\n",
        "        # Get the text from the 'review' column\n",
        "        text = row['review']\n",
        "        # Get the label (0 for negative, 1 for positive)\n",
        "        label = row['label']\n",
        "\n",
        "        # Create the annotations dictionary\n",
        "        if label == 1: # Positive review\n",
        "            annotations = {\"cats\": {\"POSITIVE\": 1.0, \"NEGATIVE\": 0.0}}\n",
        "        else: # Negative review\n",
        "            annotations = {\"cats\": {\"POSITIVE\": 0.0, \"NEGATIVE\": 1.0}}\n",
        "\n",
        "        # Add the text and its annotations to our list\n",
        "        formatted_data.append((text, annotations))\n",
        "\n",
        "    return formatted_data\n",
        "\n",
        "# Convert both our training and testing data\n",
        "train_data_spacy = format_data_for_spacy(train_df)\n",
        "test_data_spacy = format_data_for_spacy(test_df)\n",
        "\n",
        "# Print a sample to verify the format is correct\n",
        "print(\"\\nSample of formatted training data:\")\n",
        "print(train_data_spacy[0])\n",
        "\n",
        "print(f\"\\nSuccessfully converted {len(train_data_spacy)} training examples and {len(test_data_spacy)} testing examples.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jjVXZddi4bDq",
        "outputId": "d36e1332-0309-4390-f1ce-b059a5f2216b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Formatting data for spaCy...\n",
            "\n",
            "Sample of formatted training data:\n",
            "('I caught this little gem totally by accident back in 1980 or \\'81. I was at a revival theatre to see two old silly sci-fi movies. The theatre was packed full and (with no warning) they showed a bunch of sci-fi short spoofs (to get us in the mood). Most were somewhat amusing but THIS came on and, within seconds, the audience was in hysterics! The biggest laugh came when they showed \"Princess Laia\" having huge cinnamon buns instead of hair on her head. She looks at the camera, gives a grim smile and nods. That made it even funnier! You gotta see \"Chewabacca\" played by what looks like a Muppet! It was extremely silly and stupid...but I couldn\\'t stop laughing. Most of the dialogue was drowned out because of all the laughter. Also if you know \"Star Wars\" pretty well it\\'s even funnier--they deliberately poke fun at some of the dialogue. This REALLY works with an audience! A definite 10!', {'cats': {'POSITIVE': 1.0, 'NEGATIVE': 0.0}})\n",
            "\n",
            "Successfully converted 40000 training examples and 10000 testing examples.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.4. Model Initialization and Training\n",
        "\n",
        "**Code Cell Description:** This cell uses the exact training function and logic provided in the course materials. It initializes a blank spaCy model, adds the text classification component, and then trains it using the `train_data_spacy` list we created in the previous step.\n",
        "\n",
        "*   **Function Description:**\n",
        "    The `train_model(data, n_iter)` function orchestrates the training process. It iterates through the training data multiple times (epochs), showing the examples to the model and updating the model's internal weights to minimize prediction errors.\n",
        "\n",
        "*   **Syntax Explanation:**\n",
        "    *   `nlp = spacy.blank(\"en\")`: Creates a new, empty English language model. It has no pre-trained components, making it a clean slate for training.\n",
        "    *   `textcat = nlp.add_pipe(\"textcat\")`: Adds the Text Categorizer pipeline component. This is the part of the model that will learn to perform the classification.\n",
        "    *   `nlp.begin_training()`: Initializes the optimizer, which is the algorithm used to adjust the model's weights based on the errors it makes.\n",
        "    *   `random.shuffle(data)`: At the start of each epoch, the training data is shuffled. This is a critical step to ensure the model does not learn patterns based on the data's original order, which improves its ability to generalize.\n",
        "    *   `nlp.update(...)`: This is the core learning command. It shows the model a batch of examples, calculates the prediction error (loss), and updates the model's weights accordingly.\n",
        "    *   `nlp.to_disk(\"sentiment_model\")`: After training is complete, this command saves the trained model's pipeline, weights, and configuration to a directory.\n",
        "\n",
        "*   **Inputs:**\n",
        "    *   The `train_model` function is called with `train_data_spacy`, our list of 40,000 formatted training reviews.\n",
        "    *   The `n_iter` parameter is set to `2`, meaning the model will see the entire training dataset two times.\n",
        "\n",
        "*   **Outputs:**\n",
        "    *   During execution, the cell prints the \"Losses\" for each epoch. This value represents the total error the model made in that epoch. A decreasing loss value is a strong indicator that the model is learning successfully.\n",
        "    *   The final output of this cell is a folder named `sentiment_model` containing all the data for the trained model.\n",
        "\n",
        "*   **Comments and Observations:**\n",
        "  I moved on to the core task of actually training the sentiment analysis model. Following the provided structure, I first initialized a blank English spaCy model and then configured its text classification pipeline by adding the \"POSITIVE\" and \"NEGATIVE\" labels it needs to learn. I then used the provided training function, which iterates through the data for a set number of epochs, shuffles the examples, and uses the crucial nlp.update method to teach the model. I decided to run the training on my large IMDb dataset for only two epochs for this initial run, mainly because training on so much text can be very time-consuming, and I wanted to get a result quickly. After the training loop completed, I made sure to save the finished model to disk, which is a really important step so I can load and use it for making predictions later without needing to retrain it from scratch every single time."
      ],
      "metadata": {
        "id": "4tyXp8RXAdT9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Set up and train the spaCy model using the provided code structure\n",
        "\n",
        "import spacy\n",
        "from spacy.training import Example\n",
        "import random\n",
        "\n",
        "# --- START OF PROVIDED SAMPLE CODE ---\n",
        "\n",
        "# Load a blank spaCy model\n",
        "nlp = spacy.blank(\"en\")\n",
        "\n",
        "# Add the text classification pipeline\n",
        "textcat = nlp.add_pipe(\"textcat\")\n",
        "\n",
        "# Add labels for the text classification\n",
        "textcat.add_label(\"POSITIVE\")\n",
        "textcat.add_label(\"NEGATIVE\")\n",
        "\n",
        "# Training the model (using the exact function provided)\n",
        "def train_model(data, n_iter=10):\n",
        "    optimizer = nlp.begin_training()\n",
        "    for epoch in range(n_iter):\n",
        "        random.shuffle(data)\n",
        "        losses = {}\n",
        "        for text, annotations in data:\n",
        "            doc = nlp.make_doc(text)\n",
        "            example = Example.from_dict(doc, annotations)\n",
        "            nlp.update([example], drop=0.5, sgd=optimizer, losses=losses)\n",
        "        print(f\"Epoch {epoch+1}/{n_iter} - Losses: {losses}\")\n",
        "\n",
        "# --- END OF PROVIDED SAMPLE CODE ---\n",
        "\n",
        "# Now, we call the training function with OUR large dataset\n",
        "# I will use only 2 epochs (n_iter=2) for this example to make it run faster.\n",
        "print(\"Starting model training...\")\n",
        "train_model(train_data_spacy, n_iter=2)\n",
        "print(\"Training complete.\")\n",
        "\n",
        "# Save the trained model to a directory\n",
        "nlp.to_disk(\"sentiment_model\")\n",
        "print(\"Model saved to disk.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "itFaQnXx5xg5",
        "outputId": "ad6136d9-acb6-4734-f11d-0f06f6036f3e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting model training...\n",
            "Epoch 1/2 - Losses: {'textcat': 8226.893926706605}\n",
            "Epoch 2/2 - Losses: {'textcat': 6181.865608817177}\n",
            "Training complete.\n",
            "Model saved to disk.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.5. Model Performance and Evaluation\n",
        "\n",
        "**Objective:** To quantitatively assess the performance of the trained sentiment analysis model on the unseen test dataset using standard classification metrics.\n",
        "\n",
        "**Results:**\n",
        "The model achieved an overall **accuracy of 85.38%**, successfully surpassing the project's minimum performance requirement of 80%.\n",
        "\n",
        "**Analysis of Metrics:**\n",
        "\n",
        "The following metrics provide a detailed understanding of the model's performance:\n",
        "\n",
        "*   **Accuracy (85.38%):** This is the percentage of total correct predictions out of all predictions made. It provides a general measure of the model's effectiveness. While useful, it can be misleading on imbalanced datasets, which is why we also analyze the metrics below.\n",
        "\n",
        "*   **Precision:** This metric answers the question: \"Of all the predictions I made for a certain class, how many were correct?\"\n",
        "    *   For **NEGATIVE** reviews, the precision was **0.92**, indicating a high level of reliability when the model identifies a review as negative.\n",
        "    *   For **POSITIVE** reviews, the precision was **0.81**.\n",
        "\n",
        "*   **Recall:** This metric answers the question: \"Of all the actual instances of a class, how many did I correctly identify?\"\n",
        "    *   For **NEGATIVE** reviews, the recall was **0.78**. This means the model found 78% of all the negative reviews in the test set.\n",
        "    *   For **POSITIVE** reviews, the recall was **0.93**, indicating the model is very effective at identifying the majority of positive reviews.\n",
        "\n",
        "*   **F1-Score:** This is the harmonic mean of Precision and Recall, providing a single score that balances both concerns. It is particularly useful when you need a balance between finding all the positive/negative cases and not making wrong predictions. The F1-scores of **0.84 (Negative)** and **0.86 (Positive)** indicate a strong, well-balanced model.\n",
        "\n",
        "* **Conclusion:**\n",
        "In this last cell for sentiment an alysis, I evaluate how well the sentiment model actually performs on data it has never seen before. I looped through my entire test set, and for each movie review, I recorded the correct label and then used my trained nlp model to make a prediction. After collecting all the true labels and the model's corresponding predictions, I used scikit-learn to generate the key performance metrics. This provides a final, objective score, not just with the overall accuracy percentage, but with a full classification report that breaks down the precision and recall, showing me exactly how well the model learned to distinguish between positive and negative sentiments. I was able to get a higher model perfermance compared to my initial work using manual data"
      ],
      "metadata": {
        "id": "o6Dib6B_GPpS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3: Evaluate the model on the unseen test data\n",
        "\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "print(\"Evaluating model...\")\n",
        "\n",
        "# We will store the true labels and our model's predictions\n",
        "true_labels = []\n",
        "predicted_labels = []\n",
        "\n",
        "# Loop through our formatted test data\n",
        "for text, annotations in test_data_spacy:\n",
        "    # Get the true label ('POSITIVE' or 'NEGATIVE')\n",
        "    true_label = \"POSITIVE\" if annotations['cats']['POSITIVE'] == 1.0 else \"NEGATIVE\"\n",
        "    true_labels.append(true_label)\n",
        "\n",
        "    # Use the trained model to predict the sentiment of the text\n",
        "    # The 'nlp' object is our trained model in memory\n",
        "    doc = nlp(text)\n",
        "\n",
        "    # Get the predicted label by choosing the one with the higher score\n",
        "    if doc.cats['POSITIVE'] > doc.cats['NEGATIVE']:\n",
        "        predicted_labels.append(\"POSITIVE\")\n",
        "    else:\n",
        "        predicted_labels.append(\"NEGATIVE\")\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(true_labels, predicted_labels)\n",
        "print(f\"\\nModel Accuracy: {accuracy * 100:.2f}%\")\n",
        "\n",
        "# Print a detailed classification report (includes precision, recall, f1-score)\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(true_labels, predicted_labels))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EQA3lSpuFl6o",
        "outputId": "0fce6317-263d-4bdd-b9ca-bb71a63011ad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluating model...\n",
            "\n",
            "Model Accuracy: 85.38%\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    NEGATIVE       0.92      0.78      0.84      5000\n",
            "    POSITIVE       0.81      0.93      0.86      5000\n",
            "\n",
            "    accuracy                           0.85     10000\n",
            "   macro avg       0.86      0.85      0.85     10000\n",
            "weighted avg       0.86      0.85      0.85     10000\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 4: Text Summarizer\n",
        "\n"
      ],
      "metadata": {
        "id": "RD2LsjSIIgbe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.1. Text Summarizing\n",
        "\n",
        "**Objective:**\n",
        "To implement an extractive text summarizer using spaCy. This program takes a block of text and reduces it to a few key sentences by scoring each sentence based on the frequency of its most important words. This demonstrates a fundamental, non-ML approach to text summarization.\n",
        "\n",
        "---\n",
        "\n",
        "### Summarizer Implementation\n",
        "\n",
        "**Code Cell Description:**\n",
        "This code block defines and executes a text summarization pipeline. It loads a pre-trained spaCy model to help with sentence and word tokenization, and then applies a scoring algorithm to identify the most significant sentences in the text.\n",
        "\n",
        "*   **Function Description:**\n",
        "    The `summarize(text, n_sentences=2)` function is the core of this task. It takes a string of text and an integer `n_sentences` as input. It then processes the text to find the `n` most important sentences and returns them as a single, combined string.\n",
        "\n",
        "*   **Syntax Explanation:**\n",
        "    *   `doc = nlp(text)`: Processes the input text, automatically segmenting it into sentences (`doc.sents`) and tokens.\n",
        "    *   `if not token.is_stop and not token.is_punct`: This condition filters out common \"stop words\" (like 'the', 'is', 'a') and punctuation, as they typically do not carry significant meaning for summarization.\n",
        "    *   `Counter()`: A specialized dictionary from Python's `collections` library used here to efficiently count word frequencies and store sentence scores.\n",
        "    *   `sentence_scores.most_common(n_sentences)`: This is a powerful method of the `Counter` object that returns a list of the `n` most common items (in this case, the `n` sentences with the highest scores) from the counter.\n",
        "    *   `\" \".join(top_sentences)`: This joins the list of top sentences into a single string, with each sentence separated by a space.\n",
        "\n",
        "*   **Inputs:**\n",
        "    *   `user_text`: A multi-sentence block of text provided by the user via a keyboard prompt.\n",
        "    *   `n_sentences`: An optional integer parameter specifying how many sentences the final summary should contain (default is 2).\n",
        "\n",
        "*   **Outputs:**\n",
        "    *   The code prints a string (`summary`) to the console, which is the final summarized version of the input text.\n",
        "\n",
        "*   **Code Flow:**\n",
        "    1.  The pre-trained spaCy model is loaded.\n",
        "    2.  The program prompts the user to enter a block of text.\n",
        "    3.  The `summarize` function is called with the user's text.\n",
        "    4.  Inside the function, the text is processed into a `doc`.\n",
        "    5.  The frequency of each non-stop, non-punctuation word is calculated.\n",
        "    6.  Each sentence is then scored based on the sum of the frequencies of the words it contains.\n",
        "    7.  The sentences with the highest scores are selected.\n",
        "    8.  These top sentences are joined together and returned as the final summary.\n",
        "    9.  The program prints the summary to the console.\n",
        "\n",
        "*   **Comments and observations:**\n",
        "    For this final task, I explored and compared two different methods for extractive text summarization. The first approach, provided in the sample code, was a frequency-based method that works by identifying the most common important words in a text and then scoring sentences based on how many of those words they contain. To improve upon this, I implemented a second summarization function using a more advanced TF-IDF (Term Frequency-Inverse Document Frequency) algorithm, which not only considers word frequency but also how unique a word is to a particular sentence, theoretically allowing it to pinpoint more significant sentences. The most crucial part of this exercise was setting up a quantitative evaluation; I generated summaries from both methods for a sample text and then compared them against a \"gold standard\" reference summary using industry-standard metrics like ROUGE and BLEU scores. This provided an objective way to measure which summarizer was more effective, and the final interactive prompt allows me to test both algorithms on my own text to see how they perform in real-world scenarios."
      ],
      "metadata": {
        "id": "KOWGIQm4JBE4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "from collections import Counter\n",
        "from rouge_score import rouge_scorer\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import numpy as np\n",
        "\n",
        "# You may need to install the required libraries first:\n",
        "# pip install rouge-score nltk scikit-learn\n",
        "\n",
        "# Load the same pre-trained English model we used for POS tagging\n",
        "try:\n",
        "    nlp = spacy.load(\"en_core_web_sm\")\n",
        "except OSError:\n",
        "    print('Downloading language model for the first time. This may take a few minutes...')\n",
        "    from spacy.cli import download\n",
        "    download('en_core_web_sm')\n",
        "    nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# --- START OF PROVIDED SAMPLE CODE ---\n",
        "\n",
        "# Function to summarize text\n",
        "def summarize(text, n_sentences=2):\n",
        "    # Process the full text using the spaCy pipeline\n",
        "    doc = nlp(text)\n",
        "\n",
        "    # Calculate the frequency of important words (not stop words or punctuation)\n",
        "    word_frequencies = Counter()\n",
        "    for token in doc:\n",
        "        if not token.is_stop and not token.is_punct:\n",
        "            word_frequencies[token.text.lower()] += 1\n",
        "\n",
        "    # Score sentences based on the frequency of the words they contain\n",
        "    sentence_scores = Counter()\n",
        "    for sent in doc.sents:\n",
        "        for token in sent:\n",
        "            if token.text.lower() in word_frequencies:\n",
        "                sentence_scores[sent] += word_frequencies[token.text.lower()]\n",
        "\n",
        "    # Select the top N highest-scoring sentences\n",
        "    top_sentences = [sent.text for sent, score in sentence_scores.most_common(n_sentences)]\n",
        "\n",
        "    # Join the top sentences to form the final summary\n",
        "    return \" \".join(top_sentences)\n",
        "\n",
        "# --- END OF PROVIDED SAMPLE CODE ---\n",
        "\n",
        "\n",
        "# --- NEW FUNCTION FOR HIGH-SCORE SUMMARIZATION (TF-IDF Method) ---\n",
        "# This function is separate and does not modify the original.\n",
        "def summarize_for_high_scores(text, n_sentences=2):\n",
        "    doc = nlp(text)\n",
        "    sentences = [sent.text for sent in doc.sents]\n",
        "\n",
        "    if len(sentences) <= n_sentences:\n",
        "        return \" \".join(sentences)\n",
        "\n",
        "    tfidf_vectorizer = TfidfVectorizer(stop_words='english')\n",
        "    tfidf_matrix = tfidf_vectorizer.fit_transform(sentences)\n",
        "    sentence_scores = np.array(tfidf_matrix.sum(axis=1)).ravel()\n",
        "\n",
        "    top_sentence_indices = sentence_scores.argsort()[-n_sentences:][::-1]\n",
        "    top_sentence_indices.sort() # Sort to maintain original order\n",
        "\n",
        "    top_sentences = [sentences[i] for i in top_sentence_indices]\n",
        "    return \" \".join(top_sentences)\n",
        "\n",
        "\n",
        "# --- High-Score Example and Comparison ---\n",
        "example_text = (\n",
        "    \"The James Webb Space Telescope (JWST) is a space telescope designed primarily to conduct infrared astronomy. \"\n",
        "    \"As the largest optical telescope in space, its high resolution and sensitivity allow it to view objects too old, \"\n",
        "    \"distant, or faint for the Hubble Space Telescope. This has enabled a broad range of investigations across many \"\n",
        "    \"fields of astronomy and cosmology, such as observation of the first stars and the formation of the first galaxies, \"\n",
        "    \"and detailed atmospheric characterization of potentially habitable exoplanets.\"\n",
        ")\n",
        "\n",
        "example_reference_summary = (\n",
        "    \"The James Webb Space Telescope (JWST) is a space telescope designed primarily to conduct infrared astronomy. \"\n",
        "    \"This has enabled a broad range of investigations across many fields of astronomy and cosmology, such as observation of the first stars and the formation of the first galaxies, and detailed atmospheric characterization of potentially habitable exoplanets.\"\n",
        ")\n",
        "\n",
        "print(\"--- Comparing Summarization Methods on a Pre-defined Example ---\")\n",
        "print(\"\\n--- Original Text ---\")\n",
        "print(example_text)\n",
        "\n",
        "# Generate summaries from BOTH methods\n",
        "summary_from_original_code = summarize(example_text, n_sentences=2)\n",
        "summary_for_high_score = summarize_for_high_scores(example_text, n_sentences=2)\n",
        "\n",
        "\n",
        "# --- Evaluation of the Original Sample Code's Summary ---\n",
        "print(\"\\n\\n--- Summary from ORIGINAL Sample Code ---\")\n",
        "print(summary_from_original_code)\n",
        "print(\"\\n--- Evaluation Metrics for ORIGINAL Summary ---\")\n",
        "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
        "scores_original = scorer.score(example_reference_summary, summary_from_original_code)\n",
        "print(\"ROUGE Scores:\")\n",
        "for key, score in scores_original.items():\n",
        "    print(f\"{key}: F1-Score={score.fmeasure:.4f}\")\n",
        "bleu_original = sentence_bleu([example_reference_summary.split()], summary_from_original_code.split())\n",
        "print(f\"BLEU Score: {bleu_original:.4f}\")\n",
        "\n",
        "\n",
        "# --- Evaluation of the New High-Score Function's Summary ---\n",
        "print(\"\\n\\n--- Summary from NEW High-Score Function ---\")\n",
        "print(summary_for_high_score)\n",
        "print(\"\\n--- Evaluation Metrics for High-Score Summary ---\")\n",
        "scores_new = scorer.score(example_reference_summary, summary_for_high_score)\n",
        "print(\"ROUGE Scores:\")\n",
        "for key, score in scores_new.items():\n",
        "    print(f\"{key}: F1-Score={score.fmeasure:.4f}\")\n",
        "bleu_new = sentence_bleu([example_reference_summary.split()], summary_for_high_score.split())\n",
        "print(f\"BLEU Score: {bleu_new:.4f}\")\n",
        "\n",
        "\n",
        "# --- User Input Section for Your Own Testing ---\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"--- Try It Yourself ---\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "user_text = input(\"\\nEnter the text you want to summarize: \")\n",
        "if user_text.strip():\n",
        "    print(\"\\n--- Summary from ORIGINAL Sample Code ---\")\n",
        "    print(summarize(user_text, n_sentences=2))\n",
        "\n",
        "    print(\"\\n--- Summary from NEW High-Score Function ---\")\n",
        "    print(summarize_for_high_scores(user_text, n_sentences=2))\n",
        "else:\n",
        "    print(\"No text provided.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1yJ-Z3xiIg7O",
        "outputId": "5f749aaa-6887-4862-ea66-8674cb83f106"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Comparing Summarization Methods on a Pre-defined Example ---\n",
            "\n",
            "--- Original Text ---\n",
            "The James Webb Space Telescope (JWST) is a space telescope designed primarily to conduct infrared astronomy. As the largest optical telescope in space, its high resolution and sensitivity allow it to view objects too old, distant, or faint for the Hubble Space Telescope. This has enabled a broad range of investigations across many fields of astronomy and cosmology, such as observation of the first stars and the formation of the first galaxies, and detailed atmospheric characterization of potentially habitable exoplanets.\n",
            "\n",
            "\n",
            "--- Summary from ORIGINAL Sample Code ---\n",
            "As the largest optical telescope in space, its high resolution and sensitivity allow it to view objects too old, distant, or faint for the Hubble Space Telescope. The James Webb Space Telescope (JWST) is a space telescope designed primarily to conduct infrared astronomy.\n",
            "\n",
            "--- Evaluation Metrics for ORIGINAL Summary ---\n",
            "ROUGE Scores:\n",
            "rouge1: F1-Score=0.4167\n",
            "rouge2: F1-Score=0.3191\n",
            "rougeL: F1-Score=0.3333\n",
            "BLEU Score: 0.2883\n",
            "\n",
            "\n",
            "--- Summary from NEW High-Score Function ---\n",
            "As the largest optical telescope in space, its high resolution and sensitivity allow it to view objects too old, distant, or faint for the Hubble Space Telescope. This has enabled a broad range of investigations across many fields of astronomy and cosmology, such as observation of the first stars and the formation of the first galaxies, and detailed atmospheric characterization of potentially habitable exoplanets.\n",
            "\n",
            "--- Evaluation Metrics for High-Score Summary ---\n",
            "ROUGE Scores:\n",
            "rouge1: F1-Score=0.7350\n",
            "rouge2: F1-Score=0.6435\n",
            "rougeL: F1-Score=0.7009\n",
            "BLEU Score: 0.5790\n",
            "\n",
            "==================================================\n",
            "--- Try It Yourself ---\n",
            "==================================================\n",
            "\n",
            "Enter the text you want to summarize: I remember this film,it was the first film i had watched at the cinema the picture was dark in places i was very nervous it was back in 74/75 my Dad took me my brother & sister to Newbury cinema in Newbury Berkshire England. I recall the tigers and the lots of snow in the film also the appearance of Grizzly Adams actor Dan Haggery i think one of the tigers gets shot and dies. If anyone knows where to find this on DVD etc please let me know.The cinema now has been turned in a fitness club which is a very big shame as the nearest cinema now is 20 miles away, would love to hear from others who have seen this film or any other like it.\n",
            "\n",
            "--- Summary from ORIGINAL Sample Code ---\n",
            "The cinema now has been turned in a fitness club which is a very big shame as the nearest cinema now is 20 miles away, would love to hear from others who have seen this film or any other like it. I recall the tigers and the lots of snow in the film also the appearance of Grizzly Adams actor Dan Haggery i think one of the tigers gets shot and dies.\n",
            "\n",
            "--- Summary from NEW High-Score Function ---\n",
            "I recall the tigers and the lots of snow in the film also the appearance of Grizzly Adams actor Dan Haggery i think one of the tigers gets shot and dies. The cinema now has been turned in a fitness club which is a very big shame as the nearest cinema now is 20 miles away, would love to hear from others who have seen this film or any other like it.\n"
          ]
        }
      ]
    }
  ]
}